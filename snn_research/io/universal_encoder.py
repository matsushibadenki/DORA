# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/io/universal_encoder.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Universal Encoder (Offline Fix)
# ä¿®æ­£å†…å®¹: ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–ã€‚

import torch
import torch.nn as nn
import logging

logger = logging.getLogger(__name__)

# Transformersã‚’ç„¡åŠ¹åŒ–
TRANSFORMERS_AVAILABLE = False


class UniversalEncoder(nn.Module):
    """
    ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ï¼‰ã‚’çµ±ä¸€çš„ãªã‚¹ãƒ‘ã‚¤ã‚¯è¡¨ç¾ã«å¤‰æ›ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã€‚
    (ç¾åœ¨ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰/ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½)
    """

    def __init__(self, output_dim: int = 784, device: str = 'cpu', **kwargs):
        super().__init__()
        self.output_dim = output_dim
        self.device = device
        self.kwargs = kwargs  # Store extra args
        self.time_steps = kwargs.get('time_steps', 1)
        logger.info(
            f"ðŸŒ UniversalEncoder initialized (Offline Mode). Args: {kwargs}")

    def forward(self, x, modality: str = "text", **kwargs):
        # ãƒ€ãƒŸãƒ¼å®Ÿè£…: ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¹ãƒ‘ã‚¤ã‚¯ã‚’è¿”ã™
        method = kwargs.get('method', 'rate')
        batch_size = 1
        if isinstance(x, torch.Tensor):
            batch_size = x.shape[0]

        if method == 'latency':
            # Latency Coding: Larger value -> Earlier spike
            # Single spike per time window per feature
            # Normalize x to [0, 1]
            x_norm = (x - x.min()) / (x.max() - x.min() + 1e-6)

            # Convert intensity to time_step (larger intensity = smaller step index)
            spike_times = ((1.0 - x_norm) * (self.time_steps - 1)).long()

            spikes = torch.zeros(batch_size, self.time_steps,
                                 self.output_dim, device=self.device)

            # Scatter spikes
            # Create indices
            b_indices = torch.arange(batch_size, device=self.device).unsqueeze(
                1).expand(-1, self.output_dim)
            f_indices = torch.arange(self.output_dim, device=self.device).unsqueeze(
                0).expand(batch_size, -1)

            spikes[b_indices, spike_times, f_indices] = 1.0
            return spikes

        # Default / Rate / Delta (dummy implementation)
        return (torch.rand(batch_size, self.time_steps, self.output_dim) > 0.95).float().to(self.device)

    # Alias for compatibility with Brain v4 and Agents
    encode = forward
    encode_text = forward


# Legacy support / Alias
UniversalSpikeEncoder = UniversalEncoder
