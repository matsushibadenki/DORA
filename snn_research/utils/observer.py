# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/utils/observer.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Advanced Neuromorphic Observer (Layer 4 Debugging Tool)
# ç›®çš„: 
#   å®Ÿé¨“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åé›†ã«åŠ ãˆã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ç™ºç«ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã€ã‚·ã‚¹ãƒ†ãƒ å†…éƒ¨çŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã€
#   ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å‘ã‘ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ã‚’æä¾›ã™ã‚‹ã€‚

import os
import json
import time
import logging
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

class NeuromorphicObserver:
    """
    SNN OSã®ãŸã‚ã®é«˜åº¦ãªè¦³æ¸¬ãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«ã€‚
    Layer 4 (Observation) ã®å½¹å‰²ã‚’æ‹…ã„ã€å†…éƒ¨çŠ¶æ…‹ã‚’å¯è¦–åŒ–ãƒ»æ°¸ç¶šåŒ–ã™ã‚‹ã€‚
    """

    def __init__(self, base_dir: str = "benchmarks/results", experiment_name: str = "snn_experiment"):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.experiment_id = f"{experiment_name}_{self.timestamp}"
        self.save_dir = os.path.join(base_dir, self.experiment_id)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ä½œæˆ
        self.dirs = {
            "logs": os.path.join(self.save_dir, "logs"),
            "plots": os.path.join(self.save_dir, "plots"),
            "heatmaps": os.path.join(self.save_dir, "plots/heatmaps"),
            "snapshots": os.path.join(self.save_dir, "snapshots"),
            "dashboard": os.path.join(self.save_dir, "dashboard_data")
        }
        for d in self.dirs.values():
            os.makedirs(d, exist_ok=True)

        # Logger Setup
        self.logger = logging.getLogger(self.experiment_id)
        self.logger.setLevel(logging.INFO)
        fh = logging.FileHandler(os.path.join(self.dirs["logs"], "system.log"))
        fh.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
        self.logger.addHandler(fh)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
        if not any(isinstance(h, logging.StreamHandler) for h in self.logger.handlers):
            sh = logging.StreamHandler()
            sh.setFormatter(logging.Formatter('ğŸ‘ï¸ [OBSERVER] %(message)s'))
            self.logger.addHandler(sh)

        # ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢
        self.metrics: Dict[str, List[Dict[str, Any]]] = {}
        self.system_events: List[Dict[str, Any]] = []
        self.config: Dict[str, Any] = {}
        
        self.log(f"Observer initialized. ID: {self.experiment_id}")

    def log(self, message: str, level: str = "info"):
        """ãƒ­ã‚°å‡ºåŠ›"""
        if level == "info": self.logger.info(message)
        elif level == "warning": self.logger.warning(message)
        elif level == "error": self.logger.error(message)

    def set_config(self, config: Dict[str, Any]):
        """è¨­å®šã®ä¿å­˜"""
        self.config = config
        with open(os.path.join(self.save_dir, "config.json"), "w") as f:
            json.dump(config, f, indent=4, default=str)

    # --- Metrics & Events ---

    def log_metric(self, name: str, value: float, step: int, phase: str = "train"):
        """æ™‚ç³»åˆ—æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ² (Loss, Accuracy, Energyãªã©)"""
        if name not in self.metrics:
            self.metrics[name] = []
        self.metrics[name].append({
            "step": step,
            "value": float(value),
            "phase": phase,
            "timestamp": time.time()
        })

    def log_event(self, event_type: str, details: Dict[str, Any], step: int):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã®è¨˜éŒ² (Phase Change, Task Executionãªã©)"""
        event = {
            "step": step,
            "type": event_type,
            "details": details,
            "timestamp": datetime.now().isoformat()
        }
        self.system_events.append(event)
        
        # é‡å¤§ãªã‚¤ãƒ™ãƒ³ãƒˆã¯å³æ™‚ãƒ­ã‚°å‡ºåŠ›
        if event_type in ["phase_change", "error", "critical_alert"]:
            self.log(f"Event [{event_type}]: {details}")

    # --- Advanced Visualization (Layer 4) ---

    def log_heatmap(self, 
                    data: Union[np.ndarray, Any], 
                    name: str, 
                    step: int, 
                    vmin: Optional[float] = None, 
                    vmax: Optional[float] = None,
                    cmap: str = "viridis"):
        """
        è¡Œåˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡ã¿ã€æ³¨æ„ãƒãƒƒãƒ—ã€ç™ºç«é »åº¦ãªã©ï¼‰ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
        """
        # Tensor/Listå¤‰æ›
        if hasattr(data, 'detach'):
            data = data.detach().cpu().numpy()
        elif isinstance(data, list):
            data = np.array(data)
            
        if data.ndim != 2:
            # 2æ¬¡å…ƒã§ãªã„å ´åˆã¯é©å½“ã«æ•´å½¢ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—
            if data.ndim == 1:
                data = data.reshape(1, -1)
            else:
                return # 3æ¬¡å…ƒä»¥ä¸Šã¯ä»Šã®ã¨ã“ã‚æœªå¯¾å¿œï¼ˆã‚¹ãƒ©ã‚¤ã‚¹ãŒå¿…è¦ï¼‰

        plt.figure(figsize=(10, 8))
        sns.heatmap(data, cmap=cmap, vmin=vmin, vmax=vmax, square=False)
        plt.title(f"{name} (Step {step})")
        
        filename = f"{name}_step_{step:06d}.png"
        save_path = os.path.join(self.dirs["heatmaps"], filename)
        plt.savefig(save_path, bbox_inches='tight')
        plt.close()

    def snapshot_system_state(self, scheduler_status: Dict, brain_status: Dict, step: int):
        """
        OSã¨Brainã®å…¨çŠ¶æ…‹ã‚’ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¨ã—ã¦JSONä¿å­˜ã™ã‚‹ã€‚
        ãƒ‡ãƒãƒƒã‚¬ã§æ™‚ç³»åˆ—å†ç”Ÿã™ã‚‹ãŸã‚ã«ä½¿ç”¨å¯èƒ½ã€‚
        """
        snapshot = {
            "step": step,
            "timestamp": datetime.now().isoformat(),
            "scheduler": scheduler_status,
            "brain": brain_status,
            # æœ€æ–°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã‚’å«ã‚ã‚‹
            "latest_metrics": {k: v[-1]["value"] for k, v in self.metrics.items() if v}
        }
        
        filename = f"state_step_{step:06d}.json"
        with open(os.path.join(self.dirs["snapshots"], filename), "w") as f:
            json.dump(snapshot, f, indent=2, default=str)

    # --- Reporting ---

    def save_results(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        # Metrics
        with open(os.path.join(self.save_dir, "metrics.json"), "w") as f:
            json.dump(self.metrics, f, indent=4)
        
        # Events
        with open(os.path.join(self.save_dir, "system_events.json"), "w") as f:
            json.dump(self.system_events, f, indent=4)
            
        self.log(f"ğŸ’¾ All data saved to {self.save_dir}")

    def plot_learning_curve(self, metric_names: Optional[List[str]] = None):
        """å­¦ç¿’æ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        if not self.metrics: return
        
        target_metrics = metric_names if metric_names else list(self.metrics.keys())
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå€¤ãŒã‚¹ã‚«ãƒ©ã®ã‚‚ã®ã®ã¿ï¼‰
        valid_metrics = [m for m in target_metrics if m in self.metrics and len(self.metrics[m]) > 0]
        
        if not valid_metrics: return

        plt.figure(figsize=(12, 6))
        for name in valid_metrics:
            data = self.metrics[name]
            steps = [d["step"] for d in data]
            values = [d["value"] for d in data]
            plt.plot(steps, values, label=name, alpha=0.8)

        plt.xlabel("Step")
        plt.ylabel("Value")
        plt.title(f"Experiment Metrics - {self.experiment_id}")
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.5)
        
        plt.savefig(os.path.join(self.dirs["plots"], "learning_curve.png"))
        plt.close()

    def generate_dashboard_data(self):
        """
        ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ï¼ˆWeb UIç­‰ï¼‰ã§å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã®é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        dashboard_summary = {
            "experiment_id": self.experiment_id,
            "duration": "Running...", 
            "metrics_summary": {},
            "event_log_tail": self.system_events[-50:] if self.system_events else []
        }
        
        for name, data in self.metrics.items():
            if data:
                values = [d["value"] for d in data]
                dashboard_summary["metrics_summary"][name] = {
                    "last": values[-1],
                    "min": min(values),
                    "max": max(values),
                    "avg": sum(values) / len(values)
                }
        
        with open(os.path.join(self.dirs["dashboard"], "summary.json"), "w") as f:
            json.dump(dashboard_summary, f, indent=2)

    def summary(self) -> str:
        lines = [f"--- Observer Summary: {self.experiment_id} ---"]
        for name, data in self.metrics.items():
            if data:
                last = data[-1]["value"]
                lines.append(f"{name}: {last:.4f}")
        return "\n".join(lines)