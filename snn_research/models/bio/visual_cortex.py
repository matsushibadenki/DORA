# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/models/bio/visual_cortex.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Bio-Inspired Visual Cortex Model (Dynamic Shape Support)
# ç›®çš„ãƒ»å†…å®¹:
#   éœŠé•·é¡ã®è¦–è¦šé‡ï¼ˆV1, V2, V4, ITï¼‰ã‚’æ¨¡ã—ãŸéšå±¤å‹SNNãƒ¢ãƒ‡ãƒ«ã€‚
#   å…¥åŠ›æ¬¡å…ƒã‚„ãƒãƒ£ãƒãƒ«æ•°ã‚’å‹•çš„ã«è¨­å®šå¯èƒ½ã«ã—ã€æ™‚ç³»åˆ—å…¥åŠ›(Video)ã¨é™æ­¢ç”»å…¥åŠ›(Static)ã®ä¸¡æ–¹ã«å¯¾å¿œã€‚

from __future__ import annotations

import torch
import torch.nn as nn
from typing import Dict, Any, List, Optional, Tuple, Union

from snn_research.core.base import BaseModel
from snn_research.core.networks.sequential_snn_network import SequentialSNN
from snn_research.core.layers.lif_layer import LIFLayer
import logging

logger = logging.getLogger(__name__)

class VisualCortex(BaseModel):
    """
    ç”Ÿç‰©å­¦çš„è¦–è¦šé‡ãƒ¢ãƒ‡ãƒ«ã€‚
    Retina -> V1 -> V2 -> V4 -> IT ã®éšå±¤å‡¦ç†ã‚’è¡Œã†ã€‚
    """

    def __init__(
        self,
        input_shape: Tuple[int, int] = (28, 28), 
        in_channels: int = 1,
        base_channels: int = 64, 
        time_steps: int = 10,
        neuron_params: Optional[Dict[str, Any]] = None,
        **kwargs: Any
    ) -> None:
        super().__init__()
        
        self.time_steps = time_steps
        self.input_shape = input_shape
        self.in_channels = in_channels
        
        # å…¥åŠ›æ¬¡å…ƒã®è¨ˆç®— (H * W * C)
        flat_input_dim = input_shape[0] * input_shape[1] * in_channels
        
        # å„å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°è¨­å®š
        v1_dim = base_channels * 2
        v2_dim = base_channels * 4
        v4_dim = base_channels * 6
        it_dim = base_channels * 8 

        # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³è¨­å®š
        params = neuron_params or {}
        lif_config = {
            "decay": 0.9,
            "threshold": params.get("base_threshold", 1.0),
            "v_reset": 0.0,
            "tau_mem": params.get("tau_mem", 20.0)
        }
        lif_config.update(kwargs.get("lif_config", {}))

        # éšå±¤ã®æ§‹ç¯‰
        self.pathway = SequentialSNN([
            # V1: ã‚¨ãƒƒã‚¸æ¤œå‡ºãƒ»åŸºæœ¬ç‰¹å¾´
            LIFLayer(input_features=flat_input_dim, neurons=v1_dim, name="V1", **lif_config),
            
            # V2: ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ»è¤‡é›‘ãªå½¢çŠ¶
            LIFLayer(input_features=v1_dim, neurons=v2_dim, name="V2", **lif_config),
            
            # V4: ç‰©ä½“éƒ¨åˆ†ãƒ»è‰²
            LIFLayer(input_features=v2_dim, neurons=v4_dim, name="V4", **lif_config),
            
            # IT: ç‰©ä½“å…¨ä½“ãƒ»æ¦‚å¿µ
            LIFLayer(input_features=v4_dim, neurons=it_dim, name="IT", **lif_config)
        ])

        logger.info(f"ğŸ‘ï¸ VisualCortex initialized: Input({flat_input_dim}) -> V1({v1_dim}) -> IT({it_dim})")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¦–è¦šå‡¦ç†ã®å®Ÿè¡Œã€‚
        Args:
            x: 
              - Static Image: [Batch, Channels, Height, Width]
              - Video: [Batch, Time, Channels, Height, Width]
        Returns:
            torch.Tensor: [Batch, Time, Features] (ITå±¤ã®æ´»å‹•)
        """
        batch_size = x.shape[0]
        
        # å…¥åŠ›ã®å½¢çŠ¶ç¢ºèªã¨å‰å‡¦ç†
        if x.dim() == 5:
            # Video: [Batch, Time, C, H, W]
            time_steps = x.shape[1]
            # å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ•ãƒ©ãƒƒãƒˆåŒ–: [Batch, Time, Features]
            x_flat = x.view(batch_size, time_steps, -1)
            is_video = True
        elif x.dim() == 4:
            # Static Image: [Batch, C, H, W]
            time_steps = self.time_steps
            # ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¦å…¥åŠ›ã‚’ç”¨æ„: [Batch, Features]
            input_flat = x.view(batch_size, -1)
            x_flat = input_flat
            is_video = False
        else:
            # æ—¢ã«ãƒ•ãƒ©ãƒƒãƒˆãªã©ã®å ´åˆ
            if x.dim() == 2:
                time_steps = self.time_steps
                x_flat = x
                is_video = False
            elif x.dim() == 3:
                time_steps = x.shape[1]
                x_flat = x
                is_video = True
            else:
                raise ValueError(f"Unsupported input shape: {x.shape}")

        outputs = []
        
        # æ™‚é–“æ–¹å‘ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†
        for t in range(time_steps):
            # ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ã‚’å–å¾—
            if is_video:
                current_input = x_flat[:, t, :]
            else:
                current_input = x_flat # Staticã®å ´åˆã¯åŒã˜å…¥åŠ›ã‚’ç¶™ç¶šæ³¨å…¥

            # é †ä¼æ’­
            step_output = self.pathway(current_input)
            outputs.append(step_output)

        # æ™‚é–“æ–¹å‘ã«ã‚¹ã‚¿ãƒƒã‚¯: [Batch, Time, Features]
        output_stack = torch.stack(outputs, dim=1)
        
        return output_stack

    def reset_state(self) -> None:
        self.pathway.reset_state()