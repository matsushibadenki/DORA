# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/sleep_consolidation.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Sleep Consolidation & Learning Manager (SDFT Enhanced)
# ç›®çš„ãƒ»å†…å®¹:
#   - ç¡çœ ãƒ•ã‚§ãƒ¼ã‚ºã«ãŠã‘ã‚‹è„³ã®ç‰©ç†çš„ãƒ»è«–ç†çš„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã€‚
#   - Synaptic Pruning (ç‰©ç†çš„ãªçµåˆã®å‰Šé™¤)
#   - Memory Consolidation (SDFTã«ã‚ˆã‚‹è¨˜æ†¶å®šç€)

import torch
import torch.nn as nn
import logging
import time
import random
from typing import Dict, Any, Optional, List

# [NEW] SDFTç”¨ã®ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# å¾ªç’°å‚ç…§ã‚’é¿ã‘ã‚‹ãŸã‚TYPE_CHECKINGç­‰ã§ã®å¯¾å¿œãŒæœ›ã¾ã—ã„ãŒã€ã“ã“ã§ã¯ç°¡æ˜“çš„ã«é…ç½®
try:
    from snn_research.distillation.thought_distiller import ThoughtDistillationManager, SymbolicTeacher
except ImportError:
    ThoughtDistillationManager = Any  # type: ignore
    SymbolicTeacher = Any  # type: ignore

logger = logging.getLogger(__name__)


class SleepConsolidator:
    """
    Manages structural plasticity and memory consolidation during sleep cycles.
    Implements SDFT (Self-Distillation Fine-Tuning) for continual learning.
    """

    def __init__(self, substrate: Optional[nn.Module] = None, **kwargs: Any):
        if substrate is None:
            substrate = kwargs.get("target_brain_model")

        if substrate is None:
            logger.warning("SleepConsolidator initialized without substrate!")
            substrate = nn.Module()

        self.substrate = substrate
        
        # SDFTç”¨ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        # å®Ÿéš›ã«ã¯DIã‚³ãƒ³ãƒ†ãƒŠ(containers.py)ã‹ã‚‰æ³¨å…¥ã•ã‚Œã‚‹ã¹ã
        self.teacher = SymbolicTeacher()
        self.distiller = ThoughtDistillationManager(self.substrate, self.teacher)
        
        # çŸ­æœŸè¨˜æ†¶ï¼ˆãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒãƒƒãƒ•ã‚¡ï¼‰
        self.episodic_buffer: List[Dict[str, Any]] = []

    def add_episode(self, episode: Dict[str, Any]):
        """æ´»å‹•ä¸­ã«å¾—ã‚‰ã‚ŒãŸè‰¯è³ªãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰(Input, Chain, Answer)ã‚’ä¿å­˜"""
        self.episodic_buffer.append(episode)
        # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.episodic_buffer) > 100:
            self.episodic_buffer.pop(0)

    def perform_sleep_cycle(self, cycle_count: int = 1, duration_cycles: Optional[int] = None) -> Dict[str, int]:
        """Legacy alias for perform_maintenance."""
        cycles = duration_cycles if duration_cycles is not None else cycle_count
        return self.perform_maintenance(cycles)

    def perform_maintenance(self, cycle_count: int) -> Dict[str, int]:
        """
        ç¡çœ ä¸­ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        stats = {"pruned": 0, "created": 0, "learned_samples": 0}

        # 1. ç‰©ç†çš„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (10ã‚µã‚¤ã‚¯ãƒ«ã«1å›å®Ÿè¡Œ)
        if cycle_count % 10 == 0:
            stats["pruned"] = self._synaptic_pruning()
            stats["created"] = self._synaptogenesis()

        # 2. è«–ç†çš„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (SDFT / Memory Consolidation)
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒååˆ†ã«æºœã¾ã£ã¦ã„ã‚‹å ´åˆã€å¤¢ã‚’è¦‹ã‚‹ (Dreaming/SDFT)
        if len(self.episodic_buffer) >= 3:
            stats["learned_samples"] = self._perform_sdft_dreaming()
        else:
            # å¾“æ¥ãƒ¢ãƒ¼ãƒ‰ (LoRA Mock)
            stats["learned_samples"] = self._run_lora_training()

        return stats

    def _synaptic_pruning(self, threshold: float = 0.05) -> int:
        """å¼±ã„ã‚·ãƒŠãƒ—ã‚¹çµåˆã‚’ç‰©ç†çš„ã«åˆ‡æ–­ï¼ˆã‚¼ãƒ­åŒ–ï¼‰ã™ã‚‹"""
        pruned_count = 0
        for name, param in self.substrate.named_parameters():
            if "weight" in name and param.dim() > 1:
                mask = torch.abs(param.data) > threshold
                total_synapses = param.numel()
                current_active = int(
                    (torch.abs(param.data) > 1e-6).sum().item())
                new_active = int(mask.sum().item())
                pruned_count += (current_active - new_active)
                param.data *= mask.float()
        return pruned_count

    def _synaptogenesis(self, birth_rate: float = 0.01) -> int:
        """æ¥ç¶šã•ã‚Œã¦ã„ãªã„ç®‡æ‰€ã«æ–°ã—ã„ã‚·ãƒŠãƒ—ã‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã™ã‚‹"""
        created_count = 0
        for name, param in self.substrate.named_parameters():
            if "weight" in name and param.dim() > 1:
                zero_mask = (torch.abs(param.data) < 1e-6)
                birth_mask = (torch.rand_like(param.data)
                              < birth_rate) & zero_mask
                new_connections = torch.randn_like(param.data) * 0.1
                param.data += new_connections * birth_mask.float()
                created_count += int(birth_mask.sum().item())
        return created_count

    def _run_lora_training(self) -> int:
        """å¾“æ¥ã®ç¡çœ å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ (Mock)"""
        # logger.debug("ğŸ’¤ Deep sleep (No dreams)...")
        time.sleep(0.05) 
        return 0

    def _perform_sdft_dreaming(self) -> int:
        """
        [SDFT Implementation] ç¡çœ ä¸­ã®è‡ªå·±è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ (Dreaming)ã€‚
        éå»ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰(Demonstrations)ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã³ã€ãã‚Œã‚’å…ƒã«
        æ–°ã—ã„å•é¡Œï¼ˆä»®æƒ³çš„ãªçŠ¶æ³ï¼‰ã«å¯¾ã™ã‚‹æ¨è«–ã‚’è¡Œã„ã€å­¦ç¿’ã™ã‚‹ã€‚
        """
        logger.info("ğŸ¦„ Dreaming with SDFT (Self-Distillation)...")
        
        # 1. ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (éå»ã®è¨˜æ†¶)
        demos = random.sample(self.episodic_buffer, min(len(self.episodic_buffer), 3))
        
        # 2. æ–°ã—ã„å•é¡Œã®ç”Ÿæˆ (ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€éå»ã®å•é¡Œã®æ•°å€¤ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å¤‰å½¢ã™ã‚‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)
        # æœ¬æ¥ã¯Generative ModelãŒæ–°ã—ã„å•é¡Œã‚’ç”Ÿæˆã™ã‚‹
        seed_problem = random.choice(self.episodic_buffer)["input"]
        new_problems = [self._mutate_problem(seed_problem) for _ in range(5)]
        
        # 3. SDFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ (TeacherãŒICLã§è§£ã)
        sdft_dataset = self.distiller.generate_sdft_dataset(new_problems, demos)
        
        # 4. è’¸ç•™ (System 1ã®æ›´æ–°)
        if sdft_dataset:
            self.distiller.distill(sdft_dataset, epochs=1)
            
        return len(sdft_dataset)

    def _mutate_problem(self, problem: str) -> str:
        """
        ãƒ‡ãƒ¢ç”¨: ç®—æ•°å•é¡Œã®æ•°å€¤ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å¤‰æ›´ã—ã¦æ–°ã—ã„å•é¡Œã‚’ç”Ÿæˆã™ã‚‹ã€‚
        Ex: "15 + 27" -> "12 + 30"
        """
        try:
            parts = problem.replace("?", "").split("+")
            a = int(parts[0].strip())
            b = int(parts[1].strip())
            
            new_a = max(1, a + random.randint(-5, 5))
            new_b = max(1, b + random.randint(-5, 5))
            return f"{new_a} + {new_b}"
        except:
            return "10 + 10"