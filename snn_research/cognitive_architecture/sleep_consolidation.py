# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/sleep_consolidation.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Sleep Consolidation & Learning Manager
# ç›®çš„ãƒ»å†…å®¹:
#   - ç¡çœ ãƒ•ã‚§ãƒ¼ã‚ºã«ãŠã‘ã‚‹è„³ã®ç‰©ç†çš„ãƒ»è«–ç†çš„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã€‚
#   - Synaptic Pruning (ç‰©ç†çš„ãªçµåˆã®å‰Šé™¤)
#   - Memory Consolidation (LoRAç­‰ã«ã‚ˆã‚‹è¨˜æ†¶ã®å®šç€)

import torch
import torch.nn as nn
import logging
import time
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


class SleepConsolidator:
    """
    Manages structural plasticity and memory consolidation during sleep cycles.
    """

    def __init__(self, substrate: Optional[nn.Module] = None, **kwargs: Any):
        if substrate is None:
            substrate = kwargs.get("target_brain_model")

        if substrate is None:
            logger.warning("SleepConsolidator initialized without substrate!")
            substrate = nn.Module()

        self.substrate = substrate

    def perform_sleep_cycle(self, cycle_count: int = 1, duration_cycles: Optional[int] = None) -> Dict[str, int]:
        """Legacy alias for perform_maintenance."""
        cycles = duration_cycles if duration_cycles is not None else cycle_count
        return self.perform_maintenance(cycles)

    def perform_maintenance(self, cycle_count: int) -> Dict[str, int]:
        """
        ç¡çœ ä¸­ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        stats = {"pruned": 0, "created": 0, "learned_samples": 0}

        # 1. ç‰©ç†çš„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (10ã‚µã‚¤ã‚¯ãƒ«ã«1å›å®Ÿè¡Œ)
        if cycle_count % 10 == 0:
            stats["pruned"] = self._synaptic_pruning()
            stats["created"] = self._synaptogenesis()

        # 2. è«–ç†çš„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ (è¨˜æ†¶ã®å­¦ç¿’)
        # æ¯å›å®Ÿè¡Œã€ã‚‚ã—ãã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«ä½™è£•ãŒã‚ã‚‹æ™‚ã«å®Ÿè¡Œ
        stats["learned_samples"] = self._run_lora_training()

        return stats

    def _synaptic_pruning(self, threshold: float = 0.05) -> int:
        """å¼±ã„ã‚·ãƒŠãƒ—ã‚¹çµåˆã‚’ç‰©ç†çš„ã«åˆ‡æ–­ï¼ˆã‚¼ãƒ­åŒ–ï¼‰ã™ã‚‹"""
        pruned_count = 0
        for name, param in self.substrate.named_parameters():
            if "weight" in name and param.dim() > 1:
                # é‡ã¿ã®çµ¶å¯¾å€¤ãŒé–¾å€¤ä»¥ä¸‹ã®ã‚‚ã®ã‚’ãƒã‚¹ã‚¯
                mask = torch.abs(param.data) > threshold

                # çµ±è¨ˆ
                total_synapses = param.numel()
                current_active = int(
                    (torch.abs(param.data) > 1e-6).sum().item())
                new_active = int(mask.sum().item())
                pruned_count += (current_active - new_active)

                # é©ç”¨
                param.data *= mask.float()
        return pruned_count

    def _synaptogenesis(self, birth_rate: float = 0.01) -> int:
        """æ¥ç¶šã•ã‚Œã¦ã„ãªã„ç®‡æ‰€ã«æ–°ã—ã„ã‚·ãƒŠãƒ—ã‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã™ã‚‹"""
        created_count = 0
        for name, param in self.substrate.named_parameters():
            if "weight" in name and param.dim() > 1:
                # ç¾åœ¨æ¥ç¶šãŒãªã„ç®‡æ‰€ (Zero weights)
                zero_mask = (torch.abs(param.data) < 1e-6)

                # ç”Ÿæˆç¢ºç‡ã«åŸºã¥ããƒã‚¹ã‚¯
                birth_mask = (torch.rand_like(param.data)
                              < birth_rate) & zero_mask

                # æ–°ã—ã„é‡ã¿ã®åˆæœŸåŒ–ï¼ˆå°ã•ãªãƒ©ãƒ³ãƒ€ãƒ å€¤ï¼‰
                new_connections = torch.randn_like(param.data) * 0.1

                # é©ç”¨
                param.data += new_connections * birth_mask.float()

                created_count += int(birth_mask.sum().item())
        return created_count

    def _run_lora_training(self) -> int:
        """
        [NEW] ç¡çœ å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ (Dreaming)
        çŸ­æœŸè¨˜æ†¶ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰é‡è¦ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–ã‚Šå‡ºã—ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ç­‰ã«è¿½åŠ å­¦ç¿’ã‚’è¡Œã†ã€‚
        ç¾åœ¨ã¯å‹•ä½œæ¤œè¨¼ç”¨ã®ãƒ¢ãƒƒã‚¯å®Ÿè£…ã€‚
        """
        logger.info("ğŸ’¤ Dreaming... (Running background learning task)")
        
        # æœ¬æ¥ã¯ã“ã“ã§:
        # 1. Hippocampus.get_replay_buffer()
        # 2. Lossè¨ˆç®—ã¨Backward()
        # 3. Optimizer.step()
        
        # å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆæ·±ã„çœ ã‚Šï¼‰
        time.sleep(0.1) 
        
        # å­¦ç¿’ã—ãŸã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¿”ã™
        return 16 # Batch size dummy