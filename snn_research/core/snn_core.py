# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/core/snn_core.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Spiking Neural Substrate (The Kernel) Refactored
# ç›®çš„ãƒ»å†…å®¹:
#   Neuromorphic OSã®ä¸­æ ¸ã¨ãªã‚‹ç¥çµŒåŸºç›¤ã‚¯ãƒ©ã‚¹ã€‚
#   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£(Groups)ã¨ã‚·ãƒŠãƒ—ã‚¹çµåˆ(Projections)ã®ç®¡ç†ã€ãŠã‚ˆã³
#   ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçµ±åˆãƒ»ç™ºç«ãƒ»å¯å¡‘æ€§ï¼‰ã®ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œã‚’è¡Œã†ã€‚

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional, cast

import torch
import torch.nn as nn
from torch import Tensor

# çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ä½¿ç”¨ï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹é€ ãŒå‰æï¼‰
from snn_research.core.neurons.lif_neuron import LIFNeuron
from snn_research.learning_rules.base_rule import PlasticityRule

logger = logging.getLogger(__name__)


class SynapticProjection(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£é–“ã®ã‚·ãƒŠãƒ—ã‚¹çµåˆã‚’è¡¨ç¾ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    å­¦ç¿’å‰‡ï¼ˆPlasticityRuleï¼‰ã‚’ä¿æŒã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªæ›´æ–°ã‚’é©ç”¨ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        plasticity_rule: Optional[PlasticityRule] = None
    ) -> None:
        super().__init__()
        # ãƒã‚¤ã‚¢ã‚¹ãªã—ã®ç·šå½¢å±¤ã¨ã—ã¦ã‚·ãƒŠãƒ—ã‚¹ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–
        self.synapse = nn.Linear(in_features, out_features, bias=False)
        self.plasticity_rule = plasticity_rule

        # ç›´äº¤åˆæœŸåŒ–ã«ã‚ˆã‚‹ä¿¡å·ä¼æ’­ã®å®‰å®šåŒ–
        nn.init.orthogonal_(self.synapse.weight, gain=1.4)

    def forward(self, x: Tensor) -> Tensor:
        """å…¥åŠ›ã‚¹ãƒ‘ã‚¤ã‚¯åˆ—ã«å¯¾ã™ã‚‹ã‚·ãƒŠãƒ—ã‚¹é›»æµã‚’è¨ˆç®—"""
        return self.synapse(x)

    def apply_plasticity(
        self,
        pre_spikes: Tensor,
        post_spikes: Tensor,
        **kwargs: Any
    ) -> Dict[str, Any]:
        """å­¦ç¿’å‰‡ã®é©ç”¨"""
        if self.plasticity_rule is None:
            return {}

        logs: Dict[str, Any] = {}
        with torch.no_grad():
            # plasticity_rule.update ã¯ delta_w ã¨ãƒ­ã‚°ã‚’è¿”ã™
            delta_w, logs = self.plasticity_rule.update(
                pre_spikes=pre_spikes,
                post_spikes=post_spikes,
                current_weights=self.synapse.weight.data,
                **kwargs
            )

            if delta_w is not None:
                self.synapse.weight.data += delta_w
                # é‡ã¿ã®ç™ºæ•£ã‚’é˜²ããŸã‚ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®‰å®šæ€§ç¢ºä¿ï¼‰
                self.synapse.weight.data.clamp_(-5.0, 5.0)

        return logs


class SpikingNeuralSubstrate(nn.Module):
    """
    Neuromorphic OSã®ãŸã‚ã®æ±ç”¨ç¥çµŒåŸºç›¤ï¼ˆKernelï¼‰ã€‚
    æ˜ç¤ºçš„ãªæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã¾ãŸã¯ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã§çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        config: Dict[str, Any],
        device: torch.device = torch.device('cpu'),
        **kwargs: Any
    ) -> None:
        super().__init__()
        self.config = config
        self.device = device

        self.time_step: int = 0
        self.dt: float = config.get("dt", 1.0)

        self.neuron_groups: nn.ModuleDict = nn.ModuleDict()
        self.projections: nn.ModuleDict = nn.ModuleDict()
        # ãƒˆãƒãƒ­ã‚¸ãƒ¼æƒ…å ±: {'name': str, 'src': str, 'tgt': str}
        self.topology: List[Dict[str, str]] = []

        # å‰å›ã®ã‚¹ãƒ‘ã‚¤ã‚¯çŠ¶æ…‹ï¼ˆSTDPç­‰ã§ä½¿ç”¨ï¼‰: {group_name: Tensor}
        self.prev_spikes: Dict[str, Optional[Tensor]] = {}

        logger.info("âš¡ SpikingNeuralSubstrate initialized.")

    def add_neuron_group(
        self,
        name: str,
        num_neurons: int,
        neuron_model: Optional[nn.Module] = None
    ) -> None:
        """
        ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£ï¼ˆé ˜é‡ï¼‰ã‚’è¿½åŠ ã™ã‚‹ã€‚
        """
        if neuron_model is None:
            neuron_model = LIFNeuron(
                features=num_neurons,
                tau_mem=self.config.get("tau_mem", 20.0),
                v_threshold=self.config.get("threshold", 1.0),
                dt=self.dt
            )

        # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒçŠ¶æ…‹ï¼ˆè†œé›»ä½ï¼‰ã‚’ç¶­æŒã™ã‚‹ã‚ˆã†ã«è¨­å®š
        if hasattr(neuron_model, "set_stateful"):
            # mypyç”¨ã‚­ãƒ£ã‚¹ãƒˆ: set_statefulãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒã¤ã¨ä»®å®š
            cast(Any, neuron_model).set_stateful(True)

        self.neuron_groups[name] = neuron_model.to(self.device)
        self.prev_spikes[name] = None

        logger.debug(f"  + Group added: {name} ({num_neurons} neurons)")

    def add_projection(
        self,
        name: str,
        source: str,
        target: str,
        plasticity_rule: Optional[PlasticityRule] = None
    ) -> None:
        """
        é ˜åŸŸé–“ã®æŠ•å°„ã‚’è¿½åŠ ã™ã‚‹ã€‚
        """
        if source not in self.neuron_groups or target not in self.neuron_groups:
            raise ValueError(f"Source {source} or Target {target} not found.")

        # ModuleDictã‹ã‚‰å–ã‚Šå‡ºã™éš›ã¯nn.Moduleå‹ãªã®ã§ã€å±æ€§ã‚¢ã‚¯ã‚»ã‚¹ç”¨ã«ã‚­ãƒ£ã‚¹ãƒˆãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹ãŒã€
        # ã“ã“ã§ã¯featureså±æ€§ã‚’æŒã£ã¦ã„ã‚‹ã¨ä»®å®šã—ã¦å–å¾—
        src_module = cast(Any, self.neuron_groups[source])
        tgt_module = cast(Any, self.neuron_groups[target])

        src_dim = int(src_module.features)
        tgt_dim = int(tgt_module.features)

        projection = SynapticProjection(src_dim, tgt_dim, plasticity_rule)
        self.projections[name] = projection.to(self.device)

        self.topology.append({"name": name, "src": source, "tgt": target})
        logger.debug(f"  + Projection added: {name} ({source} -> {target})")

    def get_firing_rates(self) -> Dict[str, float]:
        """
        å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£ã®å¹³å‡ç™ºç«ç‡ï¼ˆç›´è¿‘ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã‚’è¿”ã™ã€‚
        """
        rates: Dict[str, float] = {}
        for name, spikes in self.prev_spikes.items():
            if spikes is not None:
                rates[name] = float(spikes.mean().item())
            else:
                rates[name] = 0.0
        return rates

    def forward_step(
        self,
        external_inputs: Dict[str, Tensor],
        **kwargs: Any
    ) -> Dict[str, Any]:
        """
        1ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€²ã‚ã‚‹ã€‚
        """
        self.time_step += 1

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æ¨å®š
        batch_size = 1
        for inp in external_inputs.values():
            batch_size = inp.shape[0]
            break

        # å‰å›ã®ã‚¹ãƒ‘ã‚¤ã‚¯çŠ¶æ…‹ã®åˆæœŸåŒ–ï¼ˆæœªå®šç¾©ã®å ´åˆï¼‰
        self._initialize_prev_spikes_if_needed(batch_size)

        # 1. Integration: ã‚·ãƒŠãƒ—ã‚¹é›»æµã¨å¤–éƒ¨å…¥åŠ›ã®çµ±åˆ
        current_inputs = self._integrate_inputs(external_inputs, batch_size)

        # 2. Dynamics: ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³çŠ¶æ…‹æ›´æ–°ãƒ»ç™ºç«
        current_spikes = self._update_neuron_dynamics(
            current_inputs, batch_size
        )

        # 3. Plasticity: å¯å¡‘æ€§ãƒ«ãƒ¼ãƒ«ã®é©ç”¨
        self._apply_plasticity(current_spikes, **kwargs)

        # 4. Update State: çŠ¶æ…‹ã®ä¿å­˜
        # current_spikesã®å€¤ã¯Tensorã§ã‚ã‚Šã€Optional[Tensor]ã«é©åˆã™ã‚‹
        self.prev_spikes = cast(Dict[str, Optional[Tensor]], current_spikes)

        return {
            "spikes": current_spikes
        }

    def _initialize_prev_spikes_if_needed(self, batch_size: int) -> None:
        """ãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦å‰å›ã®ã‚¹ãƒ‘ã‚¤ã‚¯ãƒãƒƒãƒ•ã‚¡ã‚’åˆæœŸåŒ–"""
        for name, group in self.neuron_groups.items():
            prev = self.prev_spikes.get(name)
            if prev is None or prev.shape[0] != batch_size:
                group_module = cast(Any, group)
                num_neurons = int(group_module.features)
                self.prev_spikes[name] = torch.zeros(
                    batch_size, num_neurons, device=self.device
                )

    def _integrate_inputs(
        self,
        external_inputs: Dict[str, Tensor],
        batch_size: int
    ) -> Dict[str, Tensor]:
        """
        å†…éƒ¨çµåˆã¨å¤–éƒ¨å…¥åŠ›ã‚’çµ±åˆã—ã¦ã€å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£ã¸ã®å…¥åŠ›é›»æµã‚’è¨ˆç®—ã™ã‚‹ã€‚
        """
        current_inputs: Dict[str, Tensor] = {}

        # å†…éƒ¨çµåˆï¼ˆå†å¸°ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼‰ã‹ã‚‰ã®å…¥åŠ›
        for conn in self.topology:
            proj_name = conn['name']
            src_name = conn['src']
            tgt_name = conn['tgt']

            proj_module = self.projections[proj_name]
            # prev_spikesã¯åˆæœŸåŒ–æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ãŒä¿è¨¼ã•ã‚Œã¦ã„ã‚‹ãŸã‚ cast
            src_spikes_prev = cast(Tensor, self.prev_spikes[src_name])

            # ã‚·ãƒŠãƒ—ã‚¹ä¼é”
            synaptic_current = proj_module(src_spikes_prev)

            if tgt_name not in current_inputs:
                current_inputs[tgt_name] = synaptic_current
            else:
                current_inputs[tgt_name] = current_inputs[tgt_name] + synaptic_current

        # å¤–éƒ¨å…¥åŠ›ã®åŠ ç®—
        for group_name, inp in external_inputs.items():
            if group_name not in self.neuron_groups:
                continue

            inp = inp.to(self.device)
            if group_name not in current_inputs:
                current_inputs[group_name] = inp
            else:
                if current_inputs[group_name].shape == inp.shape:
                    current_inputs[group_name] = current_inputs[group_name] + inp
                else:
                    logger.warning(
                        f"Shape mismatch in input summation for {group_name}: "
                        f"{current_inputs[group_name].shape} vs {inp.shape}. "
                        "Ignoring external input."
                    )
        
        return current_inputs

    def _update_neuron_dynamics(
        self,
        current_inputs: Dict[str, Tensor],
        batch_size: int
    ) -> Dict[str, Tensor]:
        """
        å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£ã®çŠ¶æ…‹ã‚’æ›´æ–°ã—ã€ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        current_spikes: Dict[str, Tensor] = {}

        for name, group in self.neuron_groups.items():
            if name in current_inputs:
                input_current = current_inputs[name]
            else:
                group_module = cast(Any, group)
                num_neurons = int(group_module.features)
                input_current = torch.zeros(
                    batch_size, num_neurons, device=self.device
                )

            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ (forward)
            # å¤šãã®ãƒ¢ãƒ‡ãƒ«ã¯ (spikes, state) ã‚’è¿”ã™ãŒã€ã“ã“ã§ã¯ spikes ã®ã¿ã‚’ä½¿ç”¨
            spikes, _ = group(input_current)
            current_spikes[name] = spikes

        return current_spikes

    def _apply_plasticity(
        self,
        current_spikes: Dict[str, Tensor],
        **kwargs: Any
    ) -> None:
        """
        ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«åŸºã¥ã„ã¦å¯å¡‘æ€§ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹ã€‚
        """
        for conn in self.topology:
            proj_name = conn['name']
            src_name = conn['src']
            tgt_name = conn['tgt']

            proj_module_plastic = cast(Any, self.projections[proj_name])

            # å­¦ç¿’å‰‡ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¨ˆç®—
            if (hasattr(proj_module_plastic, 'plasticity_rule') and 
                    proj_module_plastic.plasticity_rule is not None):
                
                src_spikes_prev = cast(Tensor, self.prev_spikes[src_name])
                tgt_spikes_curr = current_spikes[tgt_name]

                proj_module_plastic.apply_plasticity(
                    pre_spikes=src_spikes_prev,
                    post_spikes=tgt_spikes_curr,
                    dt=self.dt,
                    **kwargs
                )

    def reset_state(self) -> None:
        """å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ"""
        self.time_step = 0
        self.prev_spikes = {}

        for name, group in self.neuron_groups.items():
            if hasattr(group, 'reset'):
                # mypy: resetãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚‹ã¨ä»®å®š
                cast(Any, group).reset()
            self.prev_spikes[name] = None

        logger.info("ğŸ”„ Substrate state reset.")

    def get_total_spikes(self) -> int:
        """å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¹ãƒ‘ã‚¤ã‚¯ç·æ•°ã‚’è¿”ã™ï¼ˆçµ±è¨ˆç”¨ï¼‰"""
        total = 0
        for spikes in self.prev_spikes.values():
            if spikes is not None:
                total += int(spikes.sum().item())
        return total


# --- Backward Compatibility Alias ---
SNNCore = SpikingNeuralSubstrate