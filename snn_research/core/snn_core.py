# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/core/snn_core.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Spiking Neural Substrate (The Kernel) v3.3
# ç›®çš„ãƒ»å†…å®¹:
#   Neuromorphic OSã®ä¸­æ ¸ã¨ãªã‚‹ç¥çµŒåŸºç›¤ã€‚
#   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£ã¨ã‚·ãƒŠãƒ—ã‚¹çµåˆã€ãŠã‚ˆã³å¯å¡‘æ€§ãƒ«ãƒ¼ãƒ«ã‚’ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹ã€‚
#   ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã€ŒUniversal Neuron Kernelã€ã«ç›¸å½“ã€‚

import logging
import torch
import torch.nn as nn
from typing import Dict, Any, Optional, List, Tuple

# Import Native LIFNeuron
try:
    from snn_research.core.neurons.lif_neuron import LIFNeuron
except ImportError:
    # ãƒ‘ã‚¹è§£æ±ºã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    import sys
    import os
    sys.path.append(os.path.abspath(
        os.path.join(os.path.dirname(__file__), "../../")))
    from snn_research.core.neurons.lif_neuron import LIFNeuron

from snn_research.learning_rules.base_rule import PlasticityRule

logger = logging.getLogger(__name__)


class SynapticProjection(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£é–“ã®ã‚·ãƒŠãƒ—ã‚¹çµåˆã‚’è¡¨ç¾ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    å­¦ç¿’å‰‡ï¼ˆPlasticityRuleï¼‰ã‚’ä¿æŒã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªæ›´æ–°ã‚’é©ç”¨ã™ã‚‹ã€‚
    """

    def __init__(self, in_features: int, out_features: int, plasticity_rule: Optional[PlasticityRule] = None):
        super().__init__()
        self.synapse = nn.Linear(in_features, out_features, bias=False)
        self.plasticity_rule = plasticity_rule

        # ç›´äº¤åˆæœŸåŒ–ã«ã‚ˆã‚‹ä¿¡å·ä¼æ’­ã®å®‰å®šåŒ–
        nn.init.orthogonal_(self.synapse.weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å…¥åŠ›ã‚¹ãƒ‘ã‚¤ã‚¯åˆ—ã«å¯¾ã™ã‚‹ã‚·ãƒŠãƒ—ã‚¹é›»æµã‚’è¨ˆç®—"""
        return self.synapse(x)

    def apply_plasticity(self, pre_spikes: torch.Tensor, post_spikes: torch.Tensor, **kwargs) -> Dict[str, Any]:
        """å­¦ç¿’å‰‡ã®é©ç”¨"""
        if self.plasticity_rule is None:
            return {}

        with torch.no_grad():
            # plasticity_rule.update ã¯ delta_w ã¨ãƒ­ã‚°ã‚’è¿”ã™
            delta_w, logs = self.plasticity_rule.update(
                pre_spikes=pre_spikes,
                post_spikes=post_spikes,
                current_weights=self.synapse.weight.data,
                **kwargs
            )

            if delta_w is not None:
                self.synapse.weight.data += delta_w
                # é‡ã¿ã®ç™ºæ•£ã‚’é˜²ããŸã‚ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                self.synapse.weight.data.clamp_(-5.0, 5.0)

        return logs


class SpikingNeuralSubstrate(nn.Module):
    """
    Neuromorphic OSã®ãŸã‚ã®æ±ç”¨ç¥çµŒåŸºç›¤ï¼ˆKernelï¼‰ã€‚
    æ˜ç¤ºçš„ãªæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã¾ãŸã¯ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã§çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹ã€‚
    """

    def __init__(self, config: Dict[str, Any], device: torch.device):
        super().__init__()
        self.config = config
        self.device = device

        self.time_step = 0
        self.dt = config.get("dt", 1.0)

        self.neuron_groups: nn.ModuleDict = nn.ModuleDict()
        self.projections: nn.ModuleDict = nn.ModuleDict()
        self.topology: List[Dict[str, str]] = []

        # å‰å›ã®ã‚¹ãƒ‘ã‚¤ã‚¯çŠ¶æ…‹ï¼ˆSTDPç­‰ã§ä½¿ç”¨ï¼‰
        self.prev_spikes: Dict[str, torch.Tensor] = {}

        logger.info("âš¡ SpikingNeuralSubstrate initialized.")

    def add_neuron_group(self, name: str, num_neurons: int, neuron_model: Optional[nn.Module] = None):
        """
        ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é›†å›£ï¼ˆé ˜é‡ï¼‰ã‚’è¿½åŠ ã™ã‚‹ã€‚
        """
        if neuron_model is None:
            neuron_model = LIFNeuron(
                features=num_neurons,
                tau_mem=self.config.get("tau_mem", 20.0),
                v_threshold=self.config.get("threshold", 1.0),
                dt=self.dt
            )

        # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒçŠ¶æ…‹ï¼ˆè†œé›»ä½ï¼‰ã‚’ç¶­æŒã™ã‚‹ã‚ˆã†ã«è¨­å®š
        if hasattr(neuron_model, "set_stateful"):
            neuron_model.set_stateful(True)  # type: ignore

        self.neuron_groups[name] = neuron_model.to(self.device)
        self.prev_spikes[name] = None

        logger.debug(f"  + Group added: {name} ({num_neurons} neurons)")

    def add_projection(self, name: str, source: str, target: str, plasticity_rule: Optional[PlasticityRule] = None):
        """
        é ˜åŸŸé–“ã®æŠ•å°„ã‚’è¿½åŠ ã™ã‚‹ã€‚
        """
        if source not in self.neuron_groups or target not in self.neuron_groups:
            raise ValueError(f"Source {source} or Target {target} not found.")

        src_dim = self.neuron_groups[source].features  # type: ignore
        tgt_dim = self.neuron_groups[target].features  # type: ignore

        projection = SynapticProjection(src_dim, tgt_dim, plasticity_rule)
        self.projections[name] = projection.to(self.device)

        self.topology.append({"name": name, "src": source, "tgt": target})
        logger.debug(f"  + Projection added: {name} ({source} -> {target})")

    def forward_step(self, external_inputs: Dict[str, torch.Tensor], **kwargs) -> Dict[str, torch.Tensor]:
        """
        1ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€²ã‚ã‚‹ã€‚
        kwargsã«ã¯ 'phase' (wake/sleep) ãªã©ãŒå«ã¾ã‚Œã‚‹ã€‚
        """
        self.time_step += 1

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æ¨å®šã¨åˆæœŸåŒ–
        batch_size = 1
        for inp in external_inputs.values():
            batch_size = inp.shape[0]
            break

        for name, group in self.neuron_groups.items():
            # type: ignore
            if self.prev_spikes[name] is None or self.prev_spikes[name].shape[0] != batch_size:
                num_neurons = group.features  # type: ignore
                self.prev_spikes[name] = torch.zeros(
                    batch_size, num_neurons, device=self.device)

        # 1. Integration (å…¥åŠ›é›»æµã®è¨ˆç®—)
        current_inputs: Dict[str, torch.Tensor] = {}

        # å†…éƒ¨çµåˆã‹ã‚‰ã®å…¥åŠ›
        for conn in self.topology:
            proj_name = conn['name']
            src_name = conn['src']
            tgt_name = conn['tgt']

            proj_module = self.projections[proj_name]
            src_spikes_prev = self.prev_spikes[src_name]

            # ã‚·ãƒŠãƒ—ã‚¹ä¼é”
            synaptic_current = proj_module(src_spikes_prev)

            if tgt_name not in current_inputs:
                current_inputs[tgt_name] = synaptic_current
            else:
                current_inputs[tgt_name] = current_inputs[tgt_name] + \
                    synaptic_current

        # å¤–éƒ¨å…¥åŠ›ã®åŠ ç®—
        for group_name, inp in external_inputs.items():
            if group_name in self.neuron_groups:
                inp = inp.to(self.device)
                if group_name not in current_inputs:
                    current_inputs[group_name] = inp
                else:
                    # Shape check
                    if current_inputs[group_name].shape == inp.shape:
                        current_inputs[group_name] = current_inputs[group_name] + inp
                    else:
                        logger.warning(
                            f"Shape mismatch in input summation for {group_name}: {current_inputs[group_name].shape} vs {inp.shape}. Ignoring input.")

        # 2. Dynamics (ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³çŠ¶æ…‹æ›´æ–°ãƒ»ç™ºç«)
        current_spikes: Dict[str, torch.Tensor] = {}

        for name, group in self.neuron_groups.items():
            if name in current_inputs:
                input_current = current_inputs[name]
            else:
                num_neurons = group.features  # type: ignore
                input_current = torch.zeros(
                    batch_size, num_neurons, device=self.device)

            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ
            spikes, _ = group(input_current)
            current_spikes[name] = spikes

        # 3. Plasticity (å¯å¡‘æ€§ã®é©ç”¨)
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé€šã‚Šã€ã“ã“ã¯ã€Œèª¤å·®é€†ä¼æ’­ã€ã§ã¯ãªãã€Œå±€æ‰€å‰‡ã€ã®ã¿ã§æ›´æ–°ã•ã‚Œã‚‹
        for conn in self.topology:
            proj_name = conn['name']
            src_name = conn['src']
            tgt_name = conn['tgt']

            proj_module = self.projections[proj_name]  # type: ignore

            if hasattr(proj_module, 'plasticity_rule') and proj_module.plasticity_rule is not None:
                src_spikes_prev = self.prev_spikes[src_name]
                tgt_spikes_curr = current_spikes[tgt_name]

                # ã“ã“ã§ phase ã‚„ dt ãªã©ã®æƒ…å ±ã‚’å­¦ç¿’å‰‡ã«æ¸¡ã™
                proj_module.apply_plasticity(
                    pre_spikes=src_spikes_prev,
                    post_spikes=tgt_spikes_curr,
                    dt=self.dt,
                    **kwargs
                )

        # 4. Update State
        self.prev_spikes = current_spikes

        return {
            "spikes": current_spikes
        }

    def reset_state(self):
        """å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ"""
        self.time_step = 0
        self.prev_spikes = {}

        for name, group in self.neuron_groups.items():
            if hasattr(group, 'reset'):
                group.reset()
            self.prev_spikes[name] = None

        logger.info("ğŸ”„ Substrate state reset.")


# --- Alias for Backward Compatibility ---
SNNCore = SpikingNeuralSubstrate
