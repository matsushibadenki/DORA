# snn_research/core/snn_core.py
# Title: Spiking Neural Substrate v3.12 (Universal Adapter & API Compat)
# Description: VisualCortexã‚„ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒä¾å­˜ã™ã‚‹æ—§API(add_neuron_groupç­‰)ã‚’å®Ÿè£…ã—ã€mypyã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã€‚

from __future__ import annotations
import logging
import random
import math
from typing import Any, Dict, List, Optional, cast, Union, Tuple

import torch
import torch.nn as nn
from torch import Tensor

from snn_research.hardware.event_driven_simulator import DORAKernel

logger = logging.getLogger(__name__)

class SpikingNeuralSubstrate(nn.Module):
    def __init__(self, config: Dict[str, Any], device: torch.device = torch.device('cpu'), **kwargs: Any) -> None:
        super().__init__()
        self.config = config
        self.device = device
        self.dt: float = config.get("dt", 1.0)
        self.kernel = DORAKernel(dt=self.dt)
        self.group_indices: Dict[str, Tuple[int, int]] = {}
        self.prev_spikes: Dict[str, Optional[Tensor]] = {}
        self.uncertainty_score = 0.0
        
        # [Compatibility] For users accessing .neuron_groups or .projections directly
        self._projections_registry: Dict[str, Any] = {}
        
        logger.info("âš¡ SpikingNeuralSubstrate v3.12 (Universal Adapter) initialized.")

    # --- API Compatibility Layer ---
    @property
    def neuron_groups(self) -> Dict[str, Any]:
        """æ—§APIäº’æ›: ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ã‚’Dictã¨ã—ã¦è¿”ã™"""
        # Tensorã¨ã—ã¦èª¤åˆ¤å®šã•ã‚Œãªã„ã‚ˆã†ã€æ˜Žç¤ºçš„ã«Dictã‚’è¿”ã™
        return {name: {"range": r, "size": r[1]-r[0]} for name, r in self.group_indices.items()}

    @property
    def projections(self) -> Dict[str, Any]:
        """æ—§APIäº’æ›: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿”ã™"""
        return self._projections_registry

    def add_neuron_group(self, name: str, count: int, **kwargs: Any) -> None:
        """æ—§APIäº’æ›: _create_groupã¸ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹"""
        v_thresh = kwargs.get("v_thresh", kwargs.get("threshold", 0.5))
        self._create_group(name, count, v_thresh=v_thresh)

    def add_projection(self, name: str, source: str, target: str, **kwargs: Any) -> None:
        """æ—§APIäº’æ›: æŽ¥ç¶šã‚’ä½œæˆã™ã‚‹"""
        if source not in self.group_indices or target not in self.group_indices:
            logger.warning(f"âš ï¸ Cannot connect {source} -> {target}: Group not found.")
            return
        
        src_range = self.group_indices[source]
        tgt_range = self.group_indices[target]
        src_size = src_range[1] - src_range[0]
        tgt_size = tgt_range[1] - tgt_range[0]
        
        # ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿ã§æŽ¥ç¶š (æœ¬æ¥ã¯weightå¼•æ•°ã‚’å—ã‘å–ã‚‹ã¹ãã ãŒç°¡æ˜“åŒ–)
        weight_matrix = torch.randn(src_size, tgt_size).numpy() * 0.1
        self.kernel.connect_groups(src_range, tgt_range, weight_matrix)
        self._projections_registry[name] = {"source": source, "target": target}

    def apply_plasticity_batch(self, firing_rates: Any, phase: str = "neutral") -> None:
        """æ—§APIäº’æ›: å¯å¡‘æ€§é©ç”¨ã®ã‚¹ã‚¿ãƒ–"""
        # å®Ÿéš›ã®å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã¯Kernelå†…ã«ã‚ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ãƒ­ã‚°å‡ºåŠ›ã®ã¿ã§OKã¨ã™ã‚‹ã‹ã€
        # å¿…è¦ã§ã‚ã‚Œã°Kernelã®å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã¶
        pass

    def get_total_spikes(self) -> int:
        """æ—§APIäº’æ›: ç·ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ã‚’è¿”ã™"""
        return self.kernel.total_spike_count

    # -------------------------------

    def compile(self, model: Optional[nn.Module] = None) -> None:
        if not model: return
        logger.info(f"ðŸ”¨ Compiling {type(model).__name__}...")
        self.kernel = DORAKernel(dt=self.dt)
        self.group_indices = {}
        
        # [Fix] Cast model to Any to avoid "Tensor | Module" attribute errors
        model_any: Any = model
        
        input_dim = getattr(model_any, 'dim', 128)
        self._create_group("input", input_dim, v_thresh=0.2)
        curr = "input"
        
        if hasattr(model_any, 'layers'):
            for i, layer in enumerate(model_any.layers):
                b_name = f"block_{i}"
                self._create_group(f"{b_name}_in", layer.in_proj.out_features, v_thresh=0.5)
                self.kernel.connect_groups(self.group_indices[curr], self.group_indices[f"{b_name}_in"], layer.in_proj.weight.detach().cpu().numpy())
                
                d_inner = layer.in_proj.out_features // 2
                self._create_group(f"{b_name}_out", layer.out_proj.out_features, v_thresh=0.5)
                src_range = self.group_indices[f"{b_name}_in"]
                self.kernel.connect_groups((src_range[0], src_range[0]+d_inner), self.group_indices[f"{b_name}_out"], layer.out_proj.weight.detach().cpu().numpy())
                
                self.kernel.connect_groups(self.group_indices[curr], self.group_indices[f"{b_name}_out"], torch.eye(layer.out_proj.out_features).numpy())
                curr = f"{b_name}_out"
        
        if hasattr(model_any, "output_projection") and isinstance(model_any.output_projection, nn.Linear):
            self._create_group("output", model_any.output_projection.out_features, v_thresh=0.5)
            self.kernel.connect_groups(self.group_indices[curr], self.group_indices["output"], model_any.output_projection.weight.detach().cpu().numpy())
        else:
            self.group_indices["output"] = self.group_indices[curr]
            
        logger.info(f"âœ… Compilation Successful. Neurons: {len(self.kernel.neurons)}")

    def _create_group(self, name: str, count: int, v_thresh: float):
        start, end = self.kernel.create_layer_neurons(count, layer_id=len(self.group_indices), v_thresh=v_thresh)
        self.group_indices[name] = (start, end)
        self.prev_spikes[name] = torch.zeros(1, count, device=self.device)

    def forward(self, *args, **kwargs) -> Tensor:
        """
        [Universal Fix] å¼•æ•°ã‚¨ãƒ©ãƒ¼ã‚’å›žé¿ã™ã‚‹ãŸã‚ã®ä¸‡èƒ½å—ã‘å£ã€‚
        """
        input_tensor = None
        
        if args:
            for arg in args:
                if isinstance(arg, torch.Tensor):
                    input_tensor = arg
                    break
        
        if input_tensor is None:
            input_tensor = kwargs.get('input') or kwargs.get('x')

        if input_tensor is None:
            input_tensor = torch.zeros(1, 128, device=self.device)

        res_dict = self.forward_step({"input": input_tensor})
        spikes = res_dict.get("spikes", {})
        
        if "output" in spikes and spikes["output"] is not None:
            out = spikes["output"]
        else:
            valid_keys = [k for k, v in spikes.items() if v is not None]
            if valid_keys:
                out = spikes[valid_keys[-1]]
            else:
                out = torch.zeros(1, 128, device=self.device)
            
        if out.dim() == 1:
            out = out.unsqueeze(0)
            
        return out

    def forward_step(self, ext_inputs: Dict[str, Tensor], learning: bool = True, dreaming: bool = False, **kwargs: Any) -> Dict[str, Any]:
        jitter = 0.1
        if not dreaming:
            for name, tensor in ext_inputs.items():
                if name in self.group_indices:
                    start_id, _ = self.group_indices[name]
                    t = torch.as_tensor(tensor)
                    indices = torch.nonzero(t.flatten() > 0.1).flatten().cpu().numpy()
                    self.kernel.push_input_spikes([int(idx + start_id) for idx in indices], self.kernel.current_time + jitter)
        else:
            n_len = len(self.kernel.neurons)
            if n_len > 0:
                pop = list(range(n_len))
                sample_size = min(n_len, max(1, n_len // 50))
                dream_indices = random.sample(pop, sample_size)
                self.kernel.push_input_spikes(dream_indices, self.kernel.current_time + jitter)

        counts = self.kernel.run(duration=self.dt, learning_enabled=learning)
        
        curr_spikes = {}
        for name, (s, e) in self.group_indices.items():
            spikes = torch.zeros(1, e-s, device=self.device)
            for nid, count in counts.items():
                if s <= nid < e and count > 0:
                    spikes[0, nid-s] = 1.0
            curr_spikes[name] = spikes
        
        ent = 0.0
        if "output" in curr_spikes:
            ratio = max(1e-9, min(1.0 - 1e-9, float(curr_spikes["output"].mean().item())))
            ent = -(ratio * math.log(ratio) + (1-ratio) * math.log(1-ratio))
        self.uncertainty_score = (ent + self.kernel.stats.get("surprise_index", 0.0)) / 2.0
            
        self.prev_spikes = cast(Dict[str, Optional[Tensor]], curr_spikes)
        return {"spikes": curr_spikes, "uncertainty": self.uncertainty_score}

    def sleep_process(self):
        self.kernel.apply_synaptic_scaling(factor=0.9)
        stats = self.kernel.stats
        tagged = sum(1 for n in self.kernel.neurons for s in n.outgoing_synapses if s.is_tagged)
        logger.info(f"ðŸ§  Plasticity Report: Created={stats['synapses_created']}, Pruned={stats['synapses_pruned']}, Tagged={tagged}, Synapses={stats['current_synapses']}")

    def reset_state(self):
        self.kernel.reset_state()
        self.uncertainty_score = 0.0

SNNCore = SpikingNeuralSubstrate