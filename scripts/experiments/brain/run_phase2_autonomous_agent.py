# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/experiments/brain/run_phase2_autonomous_agent.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Phase 2 Autonomous Agent Experiment (Fixed: Exploration & Entropy)
# ç›®çš„ãƒ»å†…å®¹:
#   - ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã‚’è¿½åŠ ã—ã€æ¢ç´¢èƒ½åŠ›ã‚’å‘ä¸Šã€‚
#   - é€€å±ˆ(Boredom)ã«ã‚ˆã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’å°å…¥ã—ã€ã‚¹ã‚¿ãƒƒã‚¯ï¼ˆåŒã˜å ´æ‰€ã§ã®åœæ­¢ï¼‰ã‚’å›é¿ã€‚
#   - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’å¢—ã‚„ã—ã¦å­¦ç¿’ã®åæŸã‚’ç¢ºèªã€‚

import sys
import time
import logging
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from pathlib import Path
from collections import deque
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parents[3]))

from app.containers import AppContainer
from snn_research.cognitive_architecture.artificial_brain import ArtificialBrain
from snn_research.rl_env.grid_world import GridWorldEnv

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("AutonomousAgent")

class BrainAgent:
    """
    ArtificialBrainã‚’ãƒ©ãƒƒãƒ—ã—ã€RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã—ã¦æŒ¯ã‚‹èˆã‚ã›ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(self, brain: ArtificialBrain, action_dim: int, device: torch.device):
        self.brain = brain
        self.device = device
        self.action_dim = action_dim
        
        # è„³ã®å‡ºåŠ›æ¬¡å…ƒã‚’ãƒã‚§ãƒƒã‚¯
        self.brain.reset_state()
        with torch.no_grad():
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›
            dummy_input = torch.zeros(1, 4).long().to(device)
            dummy_out = self.brain(dummy_input)
            if dummy_out.dim() > 2:
                self.input_dim = dummy_out.shape[-1]
            else:
                self.input_dim = dummy_out.shape[-1]
        self.brain.reset_state()
        
        self.policy_head = nn.Linear(self.input_dim, action_dim).to(device)
        
        self.optimizer = optim.Adam(
            list(self.brain.parameters()) + list(self.policy_head.parameters()),
            lr=1e-4
        )
        
        self.memory: List[Dict[str, Any]] = []

    def get_action(self, state_tokens: torch.Tensor) -> int:
        self.brain.train()
        
        features = self.brain(state_tokens)
        
        if features.dim() > 2:
            features = features.mean(dim=1)
            
        logits = self.policy_head(features)
        probs = torch.softmax(logits, dim=-1)
        
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚‚ä¿å­˜ã—ã¦ãŠãï¼ˆå­¦ç¿’æ™‚ã«ä½¿ç”¨ï¼‰
        self.memory.append({
            "log_prob": dist.log_prob(action),
            "entropy": dist.entropy()
        })
        
        return action.item()

    def update_policy(self, rewards: list):
        R = 0
        policy_loss = []
        # [Fix] å‹æ³¨é‡ˆã‚’è¿½åŠ 
        returns: List[float] = []
        
        gamma = 0.9
        for r in rewards[::-1]:
            R = r + gamma * R
            returns.insert(0, R)
            
        # [Fix] å¤‰æ•°åã‚’å¤‰æ›´ã—ã¦å‹å†ä»£å…¥ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ (list -> Tensor)
        returns_tensor = torch.tensor(returns).to(self.device)
        
        if len(returns_tensor) > 1:
            returns_tensor = (returns_tensor - returns_tensor.mean()) / (returns_tensor.std() + 1e-9)
            
        entropy_loss = 0
        
        for i, step_data in enumerate(self.memory):
            log_prob = step_data["log_prob"]
            entropy = step_data["entropy"]
            
            # Policy Gradient Loss
            policy_loss.append(-log_prob * returns_tensor[i])
            
            entropy_loss -= 0.05 * entropy
            
        self.optimizer.zero_grad()
        
        pg_loss = torch.stack(policy_loss).sum()
        total_loss = pg_loss + entropy_loss # åˆè¨ˆæå¤±
        
        total_loss.backward()
        
        torch.nn.utils.clip_grad_norm_(self.brain.parameters(), 1.0)
        self.optimizer.step()
        
        self.memory = []
        self.brain.reset_state()
        
        return total_loss.item()

def visualize_grid(env, agent_pos, goal_pos):
    grid = [['.' for _ in range(env.size)] for _ in range(env.size)]
    
    ax, ay = agent_pos[0].item(), agent_pos[1].item()
    gx, gy = goal_pos[0].item(), goal_pos[1].item()
    
    ax, ay = min(max(0, ax), env.size-1), min(max(0, ay), env.size-1)
    gx, gy = min(max(0, gx), env.size-1), min(max(0, gy), env.size-1)

    grid[gy][gx] = 'G'
    grid[ay][ax] = 'A'
    
    if ax == gx and ay == gy:
        grid[ay][ax] = 'ğŸ‰'

    print("-" * (env.size * 2 + 3))
    for row in reversed(grid):
        print("| " + " ".join(row) + " |")
    print("-" * (env.size * 2 + 3))


def run_experiment():
    print("\n" + "="*60)
    print("ğŸ¤– Artificial Brain Phase 2: Autonomous Agent (Enhanced)")
    print("="*60 + "\n")

    container = AppContainer()
    config_path = Path("configs/templates/base_config.yaml")
    if not config_path.exists():
        config_path = Path(__file__).resolve().parents[3] / "configs/templates/base_config.yaml"
    container.config.from_yaml(str(config_path))
    container.config.device.from_value("cpu")
    
    brain = container.artificial_brain()
    device = brain.device
    
    env_size = 5
    env = GridWorldEnv(size=env_size, max_steps=30, device=str(device)) # MaxStepå°‘ã—å¢—åŠ 
    
    agent = BrainAgent(brain, action_dim=4, device=device)
    
    print(f"âœ… Environment & Agent Ready (Grid: {env_size}x{env_size})")

    # [Fix] ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’å¢—åŠ 
    episodes = 100
    success_count = 0
    total_rewards_history = []

    for episode in range(1, episodes + 1):
        env.reset()
        
        # å‰å›ã®ä½ç½®ã‚’è¨˜æ†¶ã—ã¦ã€ç§»å‹•ã—ã¦ã„ãªã„(é€€å±ˆ)åˆ¤å®šã«ä½¿ç”¨
        last_pos_tuple = (-1, -1)
        stuck_counter = 0
        
        episode_rewards = []
        done = False
        
        # æœ€åˆã®æ•°å›ã¨ã€æˆåŠŸã—ã ã—ãŸå¾ŒåŠã‚’è¡¨ç¤º
        show_render = (episode <= 3) or (episode >= episodes - 3)
        
        if show_render:
            print(f"\nğŸ¬ Episode {episode} Start")
            visualize_grid(env, env.agent_pos, env.goal_pos)

        step_count = 0
        while not done:
            state_tokens = torch.cat([env.agent_pos, env.goal_pos]).unsqueeze(0)
            
            action = agent.get_action(state_tokens)
            next_state_vec, reward, done = env.step(action)
            
            # --- [Fix] Motivation & Boredom Logic ---
            current_pos_tuple = (env.agent_pos[0].item(), env.agent_pos[1].item())
            
            # å ´æ‰€ãŒå¤‰ã‚ã£ã¦ã„ãªã„ãªã‚‰é€€å±ˆã‚«ã‚¦ãƒ³ãƒˆå¢—åŠ 
            if current_pos_tuple == last_pos_tuple:
                stuck_counter += 1
            else:
                stuck_counter = 0
            last_pos_tuple = current_pos_tuple
            
            # å‹•æ©Ÿã‚·ã‚¹ãƒ†ãƒ ã¸çŠ¶æ…‹ãƒãƒƒã‚·ãƒ¥ï¼ˆä½ç½®ï¼‰ã‚’æ¸¡ã—ã¦é€€å±ˆåº¦ã‚’æ›´æ–°ã•ã›ã‚‹
            # Tensorã§ã¯ãªãæ–‡å­—åˆ—ã«ã—ã¦æ¸¡ã™ã“ã¨ã§ãƒãƒƒã‚·ãƒ¥åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹
            pos_str = f"{current_pos_tuple}"
            internal_state = brain.motivation_system.process(pos_str)
            
            boredom = internal_state.get("boredom", 0.0)
            
            # é€€å±ˆãƒšãƒŠãƒ«ãƒ†ã‚£: å‹•ã„ã¦ã„ãªã„ã¨ç½°ã‚’ä¸ãˆã‚‹
            boredom_penalty = 0.0
            if stuck_counter > 1:
                boredom_penalty = -0.1 * stuck_counter # åœæ»ã™ã‚Œã°ã™ã‚‹ã»ã©ç—›ã„
            
            # å ±é…¬çµ±åˆ
            total_reward = reward + boredom_penalty
            episode_rewards.append(total_reward)
            
            brain.motivation_system.update_state({"reward": float(reward)})
            
            step_count += 1
            
            if show_render:
                print(f"   Step {step_count}: Action {['Up','Down','Left','Right'][action]} -> R {reward:.2f} (Boredom: {boredom:.2f})")
                visualize_grid(env, env.agent_pos, env.goal_pos)
                # time.sleep(0.05) # é«˜é€ŸåŒ–ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆæ¨å¥¨

        # å­¦ç¿’
        loss = agent.update_policy(episode_rewards)
        
        total_score = sum(episode_rewards)
        total_rewards_history.append(total_score)
        
        # ã‚´ãƒ¼ãƒ«åˆ°é”åˆ¤å®š (å ±é…¬1.0)
        is_success = False
        if any(r >= 1.0 for r in episode_rewards): # ã©ã“ã‹ã§ã‚´ãƒ¼ãƒ«å ±é…¬ã‚’å¾—ã¦ã„ã‚Œã°OK
            is_success = True
            success_count += 1
            result_mark = "ğŸ‰ Success"
        else:
            result_mark = "ğŸ’€ Failed"
            
        logger.info(f"Episode {episode:03d} | Steps: {step_count:02d} | Score: {total_score:.2f} | Loss: {loss:.4f} | {result_mark}")
        
        if episode % 20 == 0:
            brain.sleep_cycle()

    print("\n" + "="*60)
    print(f"ğŸ“Š Experiment Result: {success_count}/{episodes} Success Rate ({(success_count/episodes)*100:.1f}%)")
    print("="*60)
    
    if success_count > 10:
        print("âœ… Improved agent demonstrates adaptive behavior!")
    else:
        print("âš ï¸ Learning is still challenging. Consider simpler task or more training.")

if __name__ == "__main__":
    run_experiment()