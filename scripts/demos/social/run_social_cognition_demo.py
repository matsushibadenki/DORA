# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/demos/social/run_social_cognition_demo.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Social Cognition Demo v2.1 (Batch Training & Stability)
# ä¿®æ­£å†…å®¹: ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã€å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã€å…¥åŠ›ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’å°Žå…¥ã—ã€åŽæŸã‚’ä¿è¨¼ã€‚

import os
import sys
import torch
import torch.nn as nn
import logging
import time
import numpy as np

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.join(os.path.dirname(__file__), "../../../"))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%H:%M:%S',
    force=True
)
logger = logging.getLogger(__name__)

from snn_research.social.theory_of_mind import TheoryOfMindEncoder

class ActorAgent:
    """ç›®çš„åœ°ã«å‘ã‹ã£ã¦ç§»å‹•ã™ã‚‹å˜ç´”ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, start_pos, target_pos):
        self.pos = np.array(start_pos, dtype=np.float32)
        self.target = np.array(target_pos, dtype=np.float32)
        self.speed = 0.05 + np.random.rand() * 0.05 # ãƒ©ãƒ³ãƒ€ãƒ ãªé€Ÿåº¦
        self.history = []

    def step(self):
        direction = self.target - self.pos
        dist = np.linalg.norm(direction)
        if dist > self.speed:
            move = (direction / dist) * self.speed
            self.pos += move
        else:
            self.pos = self.target.copy()

        self.history.append(self.pos.copy())
        if len(self.history) > 16:
            self.history.pop(0)

    def get_trajectory(self):
        traj = np.array(self.history)
        if len(traj) < 16:
            pad_len = 16 - len(traj)
            # å…ˆé ­ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆé–‹å§‹åœ°ç‚¹ã«ç•™ã¾ã£ã¦ã„ã‚‹ã¨ã¿ãªã™ï¼‰
            pad = np.tile(traj[0], (pad_len, 1))
            traj = np.vstack([pad, traj])
        
        # [Feature Engineering] 
        # çµ¶å¯¾åº§æ¨™ã ã‘ã§ãªãã€ç›¸å¯¾ç§»å‹•é‡ï¼ˆé€Ÿåº¦ï¼‰ã‚‚è¨ˆç®—ã—ã¦å…¥åŠ›æƒ…å ±é‡ã‚’å¢—ã‚„ã™ã“ã¨ãŒæœ›ã¾ã—ã„ãŒã€
        # ä»Šå›žã¯ãƒ¢ãƒ‡ãƒ«å…¥åŠ›æ¬¡å…ƒã‚’å¤‰ãˆãšã«å®‰å®šåŒ–ã•ã›ã‚‹ãŸã‚ã€åº§æ¨™ã®ã¿ã¨ã™ã‚‹ã€‚
        return torch.tensor(traj, dtype=torch.float32)

def generate_batch(batch_size=32, device="cpu"):
    """å­¦ç¿’ç”¨ã®ãƒŸãƒ‹ãƒãƒƒãƒã‚’ç”Ÿæˆã™ã‚‹"""
    trajectories = []
    targets = []
    
    for _ in range(batch_size):
        # ãƒ©ãƒ³ãƒ€ãƒ ãªé–‹å§‹åœ°ç‚¹ã¨çµ‚äº†åœ°ç‚¹
        start = np.random.rand(2)
        target = np.random.rand(2)
        
        actor = ActorAgent(start, target)
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¹ãƒ†ãƒƒãƒ—æ•°ã ã‘é€²ã‚ã‚‹ï¼ˆé€”ä¸­çµŒéŽã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ã™ã‚‹ï¼‰
        steps = np.random.randint(5, 20)
        for _ in range(steps):
            actor.step()
            
        trajectories.append(actor.get_trajectory())
        targets.append(torch.tensor(target, dtype=torch.float32))
        
    # Stack: [Batch, Time, Dim]
    batch_traj = torch.stack(trajectories).to(device)
    batch_target = torch.stack(targets).to(device)
    return batch_traj, batch_target

def run_social_demo():
    print("""
    =======================================================
       ðŸ¤ SOCIAL COGNITION DEMO v2.1 (Batch Training) ðŸ¤
    =======================================================
    """)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"âš™ï¸ Running on {device.upper()}")

    # 1. Initialize ToM Engine
    tom_engine = TheoryOfMindEncoder(
        input_dim=2,
        hidden_dim=128,
        intent_dim=2,
        model_type="gru", 
        history_len=16
    ).to(device)

    # å­¦ç¿’çŽ‡ã‚’å°‘ã—ä¸‹ã’ã‚‹ï¼ˆå®‰å®šé‡è¦–ï¼‰
    optimizer = torch.optim.AdamW(tom_engine.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    logger.info("ðŸ§  Observer Agent (ToM - GRU Core) initialized.")

    # 2. Training Phase
    logger.info("ðŸŽ“ Phase 1: Observing & Learning Intentions (Batch Training)...")
    
    total_steps = 1000 # Batch updates
    start_train = time.time()
    
    tom_engine.train()
    
    for step in range(total_steps):
        # ãƒãƒƒãƒç”Ÿæˆ
        trajs, targets = generate_batch(batch_size=32, device=device)
        
        optimizer.zero_grad()
        preds = tom_engine(trajs)
        loss = criterion(preds, targets)
        loss.backward()
        
        # [Fix] å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆçˆ†ç™ºé˜²æ­¢ï¼‰
        torch.nn.utils.clip_grad_norm_(tom_engine.parameters(), max_norm=1.0)
        
        optimizer.step()

        if (step+1) % 100 == 0:
            logger.info(f"   Step {step+1}/{total_steps}: Loss = {loss.item():.6f}")

    train_time = time.time() - start_train
    logger.info(f"âœ… Training completed in {train_time:.2f}s")

    # 3. Testing Phase
    logger.info("\nðŸ”® Phase 2: Real-time Intent Prediction Test")

    # Scenario: Start Left-Top -> Goal Right-Bottom
    start_pos = [0.1, 0.9]
    real_target = [0.9, 0.1]
    actor = ActorAgent(start_pos=start_pos, target_pos=real_target)

    logger.info(f"   Actor Start: {start_pos} -> Secret Goal: {real_target}")

    tom_engine.eval()
    
    for t in range(20):
        actor.step()
        traj = actor.get_trajectory().unsqueeze(0).to(device)

        start_time = time.time()
        with torch.no_grad():
            pred = tom_engine.predict_goal(traj)
        lat = (time.time() - start_time) * 1000

        pred_pos = pred.cpu().numpy()[0]
        dist = np.linalg.norm(pred_pos - real_target)

        status = "ðŸ¤” Guessing..."
        if dist < 0.05: status = "ðŸ’¡ I KNOW!"
        elif dist < 0.15: status = "ðŸ‘€ Getting closer..."

        pos_str = f"[{actor.pos[0]:.2f}, {actor.pos[1]:.2f}]"
        pred_str = f"[{pred_pos[0]:.2f}, {pred_pos[1]:.2f}]"
        
        logger.info(
            f"   Step {t:02d}: Pos={pos_str} -> Predicted={pred_str} | Err={dist:.2f} | {status} ({lat:.2f}ms)")

        if dist < 0.05:
            logger.info("   âœ… Correctly predicted intent with high precision!")
            break

    logger.info("ðŸŽ‰ Social Cognition Demo Completed.")

if __name__ == "__main__":
    run_social_demo()