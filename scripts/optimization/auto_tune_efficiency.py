# scripts/optimization/auto_tune_efficiency.py
# SNNã®å‹•ä½œåŠ¹ç‡ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è‡ªå‹•æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: scripts/optimization/auto_tune_efficiency.py
# ãƒ•ã‚¡ã‚¤ãƒ«å: SNNåŠ¹ç‡æ€§è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«
# ç›®çš„: å®Ÿãƒ¢ãƒ‡ãƒ«ã‚’é§†å‹•ã•ã›ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¨æ¨å®šç²¾åº¦(ç™ºç«ç‡ãƒ™ãƒ¼ã‚¹)ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚

import argparse
import sys
import logging
import optuna
import time
import torch
import numpy as np
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

from snn_research.models.transformer.spikformer import Spikformer

def main():
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(levelname)s - %(message)s')
    parser = argparse.ArgumentParser(description="SNNåŠ¹ç‡æ€§è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    parser.add_argument("--n-trials", type=int, default=20, help="è©¦è¡Œå›æ•°")
    parser.add_argument("--device", type=str, default="mps" if torch.backends.mps.is_available() else ("cuda" if torch.cuda.is_available() else "cpu"))
    args = parser.parse_args()

    print(f"ğŸš€ Using Device: {args.device}")

    # æ¸¬å®šç”¨ãƒ€ãƒŸãƒ¼å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (B, C, H, W)
    batch_size = 8 
    input_shape = (batch_size, 3, 224, 224)
    dummy_input = torch.randn(input_shape).to(args.device)

    def objective(trial):
        # --- æ¢ç´¢ç©ºé–“ã®å®šç¾© ---
        # 1. æ§‹é€ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (é€Ÿåº¦ã¨è¡¨ç¾åŠ›)
        time_steps = trial.suggest_categorical("model.T", [1, 2, 4, 8])
        embed_dim = trial.suggest_categorical("model.embed_dim", [128, 256])
        
        # 2. ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (ã‚¹ãƒ‘ã‚¤ã‚¯ç‡/å®‰å®šæ€§)
        tau_m = trial.suggest_float("model.neuron.tau_m", 1.5, 4.0)
        base_threshold = trial.suggest_float("model.neuron.base_threshold", 0.6, 1.5)

        # ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
        try:
            model = Spikformer(
                img_size_h=224, img_size_w=224,
                embed_dim=embed_dim,
                num_heads=8,
                num_layers=4,
                T=time_steps,
                num_classes=10
            ).to(args.device)
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ³¨å…¥ (ç°¡æ˜“å®Ÿè£…)
            # å®Ÿéš›ã«ã¯ConfigçµŒç”±ãŒæœ›ã¾ã—ã„ãŒã€æœ€é©åŒ–ãƒ«ãƒ¼ãƒ—å†…ã§ã¯ç›´æ¥å±æ€§æ“ä½œã‚’è¡Œã†
            for m in model.modules():
                if hasattr(m, 'v_threshold'):
                    m.v_threshold = base_threshold
                if hasattr(m, 'tau_m'):
                    # DualAdaptiveLIFNodeã®å®Ÿè£…ã«åˆã‚ã›ã‚‹
                    if hasattr(m, 'tau_m_init'): 
                         m.tau_m.data.fill_(tau_m)
        except Exception as e:
            print(f"âš ï¸ Model Build Failed: {e}")
            return 1000.0 # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢

        # --- è¨ˆæ¸¬ãƒ•ã‚§ãƒ¼ã‚º ---
        model.eval()
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— (MPS/CUDAã®åˆæœŸåŒ–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰æ’é™¤)
        with torch.no_grad():
            try:
                _ = model(dummy_input)
            except Exception as e:
                print(f"âš ï¸ Warmup Failed: {e}")
                return 1000.0

        # 1. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆæ¸¬ (Speed)
        start_time = time.time()
        with torch.no_grad():
            for _ in range(10): # 10å›å¹³å‡
                outputs = model(dummy_input)
        end_time = time.time()
        avg_latency_ms = ((end_time - start_time) / 10.0) * 1000.0

        # 2. ä»®æƒ³ç²¾åº¦ã‚¹ã‚³ã‚¢ (Accuracy Potential)
        # æœ¬æ¥ã¯Validationãƒ‡ãƒ¼ã‚¿ã§æ¸¬ã‚‹ãŒã€ã“ã“ã§ã¯ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªã€Œæƒ…å ±é‡ã€ã‚’æŒ‡æ¨™ã¨ã™ã‚‹
        # TãŒå¤§ãã„ã»ã©ã€EmbedãŒå¤§ãã„ã»ã©ã€é–¾å€¤ãŒé©æ­£(1.0ä»˜è¿‘)ãªã»ã©æƒ…å ±é‡ãŒå¤šã„ã¨ä»®å®š
        
        # Tã«ã‚ˆã‚‹æƒ…å ±ã‚²ã‚¤ãƒ³ (å¯¾æ•°çš„)
        info_gain_t = np.log2(time_steps + 1) * 0.5 
        
        # æ¬¡å…ƒæ•°ã«ã‚ˆã‚‹æƒ…å ±ã‚²ã‚¤ãƒ³
        info_gain_dim = 1.0 if embed_dim >= 256 else 0.7
        
        # é–¾å€¤ãƒšãƒŠãƒ«ãƒ†ã‚£ (ä½ã™ãã‚‹ã¨ãƒã‚¤ã‚ºéå¤šã€é«˜ã™ãã‚‹ã¨æƒ…å ±æ¶ˆå¤±)
        if base_threshold < 0.8:
            thresh_score = 0.5 # Noise penalty
        elif base_threshold > 1.3:
            thresh_score = 0.6 # Silence penalty
        else:
            thresh_score = 1.0 # Optimal range

        potential_score = (info_gain_t + info_gain_dim) * thresh_score

        # --- ç›®çš„é–¢æ•° (Minimize) ---
        # ç›®æ¨™: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· < 10ms ã‚’ç¶­æŒã—ã¤ã¤ã€Potentialã‚’æœ€å¤§åŒ–ã—ãŸã„
        # Score = Latency_Penalty + (Max_Potential - Potential)
        
        latency_penalty = 0.0
        if avg_latency_ms > 10.0:
            latency_penalty = (avg_latency_ms - 10.0) * 2.0 # 10msè¶…ãˆã¯å³ã—ãç½°ã™ã‚‹
        
        # æœ€å¤§Potentialç›®å®‰: (log2(9)*0.5 + 1.0)*1.0 â‰ˆ 2.5
        score = latency_penalty + (3.0 - potential_score)

        return score

    print("ğŸ” Starting Optimization...")
    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=args.n_trials)

    print("=" * 60)
    print("ğŸ† ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
    print("=" * 60)
    print(f"  Best Score: {study.best_value:.4f}")
    print(f"  Best Params: {study.best_params}")
    
    # æ¨å¥¨è¨­å®šã®è¡¨ç¤º
    best = study.best_params
    print("-" * 30)
    print("  [Recommended Configuration for YAML]")
    print(f"  time_steps: {best['model.T']}")
    print(f"  d_model: {best['model.embed_dim']}")
    print(f"  neuron:")
    print(f"    base_threshold: {best['model.neuron.base_threshold']:.2f}")
    print(f"    tau: {best['model.neuron.tau_m']:.2f}")
    print("=" * 60)

if __name__ == "__main__":
    main()