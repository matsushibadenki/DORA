# isort: skip_file
from omegaconf import OmegaConf
import torch
import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ  (ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å‰ã«è¡Œã†)
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))


# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (sys.pathè¨­å®šå¾Œ)
try:
    from transformers import AutoTokenizer
    from torch.utils.data import DataLoader
    from snn_research.data.datasets import SimpleTextDataset
    # [FIX] Use high-level model instead of Kernel
    # from snn_research.core.snn_core import SNNCore
    from snn_research.models.transformer.spiking_rwkv import BitSpikingRWKV
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    print(f"Current sys.path: {sys.path}")
    sys.exit(1)


def main():
    print("ğŸ” SNNè©³ç´°ä¿¡å·è¨ºæ–­ (Full Forward / Fixed) ã‚’é–‹å§‹ã—ã¾ã™...")

    # 1. è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«
    # scripts/debug/../../configs -> project_root/configs
    config_path = os.path.join(os.path.dirname(os.path.dirname(
        os.path.dirname(__file__))), "configs/models/bit_rwkv_micro.yaml")

    if not os.path.exists(config_path):
        print(f"âŒ Config not found: {config_path}")
        return

    cfg = OmegaConf.load(config_path)

    print("  - Loading tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        # Padding token setting for GPT-2
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
    except Exception as e:
        print(f"âŒ Tokenizer load failed: {e}")
        return

    device = "cpu"

    print("  - Building model...")
    try:
        # [FIX] Instantiate BitSpikingRWKV directly
        # Config structure might differ, map accordingly or pass flat args if model doesn't take config obj
        # BitSpikingRWKV(vocab_size, d_model=..., num_layers=...)

        mdl_cfg = cfg.model

        # Extract params safely or use defaults
        d_model = mdl_cfg.get("d_model", 256)
        num_layers = mdl_cfg.get("num_layers", 4)
        time_steps = mdl_cfg.get("time_steps", 16)

        # If config is nested differently, adjust here.

        model = BitSpikingRWKV(
            vocab_size=len(tokenizer),
            d_model=d_model,
            num_layers=num_layers,
            time_steps=time_steps,
            # neuron_config can be passed if needed
        )

        model.to(device)
        model.eval()
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return

    # 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    data_path = os.path.join(os.path.dirname(os.path.dirname(
        os.path.dirname(__file__))), "data/smoke_test_data.jsonl")

    if not os.path.exists(data_path):
        print(f"âŒ Data not found: {data_path}")
        # Create dummy data if missing for diagnosis
        print("âš ï¸  Using dummy data instead.")
        dummy_data = True
    else:
        dummy_data = False

    if dummy_data:
        # Create minimal dummy dataset interface
        class DummyDataset:
            def __len__(self): return 1
            def __getitem__(self, idx): return (torch.randint(
                0, 1000, (16,)), torch.randint(0, 1000, (16,)))
        dataset = DummyDataset()
    else:
        dataset = SimpleTextDataset(data_path, tokenizer, max_seq_len=16)

    loader = DataLoader(dataset, batch_size=1, shuffle=False)

    try:
        batch = next(iter(loader))
        if isinstance(batch, (list, tuple)):
            input_ids = batch[0].to(device)
        elif isinstance(batch, dict):
            input_ids = batch['input_ids'].to(device)
        elif isinstance(batch, torch.Tensor):
            input_ids = batch.to(device)
        else:
            print(f"âŒ Unexpected batch type: {type(batch)}")
            return
    except StopIteration:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©ºã§ã™ã€‚")
        return

    # 3. è©³ç´°è¨ºæ–­å®Ÿè¡Œ
    print("\nğŸ“Š ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ¥ä¿¡å·è¿½è·¡:")

    # ãƒ•ãƒƒã‚¯é–¢æ•°: å…¥å‡ºåŠ›ã®çµ±è¨ˆã‚’è¡¨ç¤º (ä¿®æ­£: floatã‚­ãƒ£ã‚¹ãƒˆè¿½åŠ )
    def debug_hook(name):
        def hook(module, input, output):
            if isinstance(input, tuple):
                input = input[0]
            if isinstance(output, tuple):
                output = output[0]

            # float()ã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ã‹ã‚‰è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
            in_mean = input.float().abs().mean().item(
            ) if isinstance(input, torch.Tensor) else 0.0
            out_mean = output.float().abs().mean().item(
            ) if isinstance(output, torch.Tensor) else 0.0
            out_max = output.float().abs().max().item(
            ) if isinstance(output, torch.Tensor) else 0.0

            # ã‚¹ãƒ‘ã‚¤ã‚¯æ•° (ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å ´åˆ)
            spike_info = ""
            if "lif" in name.lower() or "neuron" in name.lower():
                if isinstance(output, torch.Tensor):
                    spike_count = output.sum().item()
                    # Only calculate rate if count > 0 to avoid noise
                    if spike_count > 0:
                        spike_rate = output.float().mean().item() * 100
                        spike_info = f" | Spikes: {int(spike_count)} (Rate: {spike_rate:.2f}%)"
                    else:
                        spike_info = " | Spikes: 0"

            print(f"  ğŸ”¹ [{name}]")
            print(f"      Input Mean: {in_mean:.6f}")
            print(
                f"      Output Mean: {out_mean:.6f} | Max: {out_max:.6f}{spike_info}")

            if out_max == 0 and "lif" not in name.lower() and "neuron" not in name.lower():
                # Embeddingå…¥åŠ›(Long)ã¯é™¤ã
                if "embedding" not in name.lower():
                    print(f"      ğŸš¨ ä¿¡å·æ¶ˆå¤±è­¦å ±: {name} ã®å‡ºåŠ›ãŒã‚¼ãƒ­ã§ã™ï¼")
        return hook

    # ä¸»è¦ãªå±¤ã«ãƒ•ãƒƒã‚¯ã‚’ç™»éŒ²
    hooks = []

    # Embedding - BitSpikingRWKV has 'embedding' ? Let's check or user generic try
    if hasattr(model, 'embedding'):
        hooks.append(model.embedding.register_forward_hook(
            debug_hook("Embedding")))

    # Layers
    if hasattr(model, 'blocks'):  # RWKV usually has blocks
        for i, layer in enumerate(model.blocks):
            # Inspect structure if needed
            hooks.append(layer.register_forward_hook(debug_hook(f"Block{i}")))

            # BitLinear if present
            if hasattr(layer, 'time_mixing'):
                # Check submodules
                pass

    # å®Ÿè¡Œ
    with torch.no_grad():
        try:
            print("\n  --- Forward Pass Start ---")
            # RWKV forward sig: forward(self, input_ids, return_spikes=False, ...)
            model(input_ids, return_spikes=True)
            print("  --- Forward Pass End ---")
        except Exception as e:
            print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

    # å¾Œå§‹æœ«
    for h in hooks:
        h.remove()
    print("\nâœ… è¨ºæ–­çµ‚äº†")


if __name__ == "__main__":
    main()
