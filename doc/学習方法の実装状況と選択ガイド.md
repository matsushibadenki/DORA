# **学習方法の実装状況と選択ガイド**

このドキュメントは、matsushibadenki/dora プロジェクトにおける各種学習アルゴリズムの実装状況を整理し、目的や制約に応じた適切な手法を選択するための指針を示します。

**更新情報 (2026-02-09):**

SARA Engine v7.4 の統合に伴い、Predictive Coding, Active Inference, および強化学習 (PPO) モジュールが大幅に強化されました。また、HDC（高次元コンピューティング）や Tsetlin Machine といった論理・連想ベースの軽量アルゴリズムが追加されました。

## **1\. 実装状況一覧表**

現在の実装レベルを以下の3段階で分類しています。

* ✅ **Core**: 中核実装済み。動作確認が取れており、推奨される。  
* ⚠️ **Experimental**: 実験的実装。特定のモデルや条件下でのみ動作、または調整中。  
* ❌ **Draft**: 枠組みのみ。ファイルは存在するがロジックが未実装、または空の状態。

| カテゴリ | 学習手法 | 実装状況 | 主要ファイル (snn\_research/) | 推奨度 |
| :---- | :---- | :---- | :---- | :---- |
| **生体模倣** | **STDP** | ✅ Core | learning\_rules/stdp.py | ★★★ |
|  | **Hebbian / BCM** | ✅ Core | learning\_rules/bcm\_rule.py | ★★☆ |
|  | **Probabilistic Hebbian** | ⚠️ Experimental | learning\_rules/probabilistic\_hebbian.py | ★☆☆ |
| **代替BP** | **Forward-Forward** | ✅ Core | training/trainers/forward\_forward.py | ★★★ |
|  | **Predictive Coding** | ✅ **SARA** | models/experimental/predictive\_coding\_model.py | ★★★ |
|  | **Surrogate Gradient** | ✅ Core | core/surrogates.py | ★★★ |
| **高次認知** | **Active Inference** | ✅ **SARA** | learning\_rules/active\_inference.py | ★★★ |
|  | **HDC (Hyperdimensional)** | ✅ Core | cognitive\_architecture/hdc\_engine.py | ★★☆ |
|  | **Causal Trace** | ⚠️ Experimental | learning\_rules/causal\_trace.py | ★☆☆ |
| **効率・論理** | **Tsetlin Machine** | ✅ Core | cognitive\_architecture/tsetlin\_machine.py | ★★★ |
| **特殊最適化** | **Spike PPO** | ✅ Core | training/rl/spike\_ppo.py | ★★☆ |
|  | **Physics-Informed** | ⚠️ Experimental | training/trainers/physics\_informed.py | ★★☆ |
|  | **Distillation** | ✅ Core | distillation/system\_distiller.py | ★★★ |

## **2\. 詳細解説とアップデート内容**

### **2.1 Hebbian / BCM Rule (シナプス閾値調整)**

* **ステータス**: ✅ Core  
* **ファイル**: learning\_rules/bcm\_rule.py  
* **内容**:  
  * Bienenstock-Cooper-Munro (BCM) 則に基づき、シナプス可塑性の閾値を動的に調整します。  
  * 単純なヘブ則で発生する「重みの爆発」を防ぎ、ニューロンの活動を一定範囲に保つホメオスタシス機能を提供。  
* **特徴**: 競合学習（Competitive Learning）において、特定のニューロンが過剰に学習するのを防ぎ、ネットワーク全体の表現力を向上させます。

### **2.2 HDC Engine (高次元連想記憶)**

* **ステータス**: ✅ Core  
* **ファイル**: cognitive\_architecture/hdc\_engine.py  
* **内容**:  
  * 数千次元のハイパーベクトルを用いて情報をシンボル化し、束縛（Binding）や重ね合わせ（Superposition）といった論理演算で概念を扱います。  
* **特徴**: **One-shot学習**（一度の提示での記憶）に極めて強く、SNNのスパイク表現を記号論理的な推論に変換するブリッジとして機能します。

### **2.3 Tsetlin Machine (論理ゲート学習)**

* **ステータス**: ✅ Core  
* **ファイル**: cognitive\_architecture/tsetlin\_machine.py  
* **内容**:  
  * 多数決論理とTsetlin Automataを用いた、乗算を必要としない学習アルゴリズム。  
* **特徴**: 従来のニューラルネットワークと比較して**消費電力が圧倒的に少なく**、FPGAや極低電力エッジデバイスへの実装に最適です。

### **2.4 Physics-Informed Learning (物理制約学習)**

* **ステータス**: ⚠️ Experimental  
* **ファイル**: training/trainers/physics\_informed.py  
* **内容**:  
  * 損失関数の中に微分方程式（重力、慣性、流体など）の制約を組み込みます。  
* **特徴**: ロボティクスの制御など、物理的な正当性が求められるタスクにおいて、データが少ない状態でも安定した挙動を学習可能です。

## **3\. 詳細選択ガイド：いつ、どの学習方法を使うべきか**

### **A. リアルタイム・エッジデバイスでの自律学習**

* **推奨**: STDP, BCM Rule, Tsetlin Machine  
* **理由**: バックプロパゲーションが不要で、局所的な計算（Local Learning）のみで完結するため。  
* **特に電力を重視する場合**: Tsetlin Machine を選択してください。

### **B. 高精度な認識とオフライン定着**

* **推奨**: Surrogate Gradient → Post-Backprop → Distillation  
* **シナリオ**:  
  1. Surrogate Gradient で高精度なモデルを作成。  
  2. 推論実行中のログを Post-Backprop で微調整。  
  3. 最終的に Distillation を使い、巨大なモデルの知識を極小SNNへ圧縮。

### **C. 少数データからの概念形成（One-shot記憶）**

* **推奨**: HDC Engine, Probabilistic Hebbian  
* **理由**: HDCは高次元空間へのマッピングにより、一度の経験を強力な連想パターンとして保持できるため。  
* **注意**: 画像のピクセルレベルの識別よりも、意味的なラベル付けやシンボル操作に適しています。

### **D. ロボティクス・動的環境での生存戦略**

* **推奨**: Active Inference (via SARA), Physics-Informed  
* **理由**: SARA Engineに統合された Active Inference は、将来の予測エラーを最小化するように行動を決定します。  
* **注意**: 計算負荷が高いため、Predictive Coding による予測エラーの抽出を前段に置くことを推奨します。

## **4\. 未実装・研究中の項目**

* **LLM-to-SNN Adaptation**: 大規模言語モデルのTransformer重みをSNNのシナプス重みに変換するコンバータ（conversion/ 内で開発中）。  
* **Quantum-Inspired SNN**: スパイクの重畳状態を量子ビットとして扱う実験的モデル（experimental/ 参照）。