# **学習方法の実装状況と選択ガイド**

このドキュメントは、matsushibadenki/dora プロジェクトにおける各種学習アルゴリズムの実装状況を整理し、目的や制約に応じた適切な手法を選択するための指針を示します。

## **1\. 実装状況一覧表**

現在の実装レベルを以下の3段階で分類しています。

* ✅ **Core**: 中核実装済み。動作確認が取れており、推奨される。  
* ⚠️ **Experimental**: 実験的実装。特定のモデルや条件下でのみ動作、または調整中。  
* ❌ **Draft**: 枠組みのみ。ファイルは存在するがロジックが未実装、または空の状態。

| カテゴリ | 学習手法 | 実装状況 | 主要ファイル (snn\_research/) | 推奨度 | 用途 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **生体模倣** | **STDP** (Spike-Timing Dependent Plasticity) | ✅ Core | core/synapse\_dynamics.py learning\_rules/stdp.py | ★★★ | 教師なし学習、特徴抽出、自己組織化 |
|  | **Hebbian / BCM** | ✅ Core | learning\_rules/probabilistic\_hebbian.py learning\_rules/bcm\_rule.py | ★★☆ | 連想記憶、恒常性維持 |
| **代替BP** | **Forward-Forward** | ✅ Core | training/trainers/forward\_forward.py learning\_rules/forward\_forward.py | ★★★ | 深層SNNの省メモリ学習、エッジデバイス向け |
|  | **Predictive Coding** | ⚠️ Exp | models/experimental/predictive\_coding\_model.py | ★★☆ | 生成モデル、異常検知、トップダウン予測 |
| **強化学習** | **Spike PPO** | ✅ Core | training/rl/spike\_ppo.py | ★★★ | 安定した方策学習、ロボット制御 |
|  | **Spike SAC** (Soft Actor-Critic) | ⚠️ Exp | training/rl/spike\_sac.py | ★★☆ | 連続値制御、サンプル効率重視の探索 |
|  | **Active Inference** | ✅ Core | learning\_rules/active\_inference.py | ★★☆ | 能動的推論、不確実性最小化による行動決定 |
| **高度トレーナー** | **Breakthrough** | ✅ Core | training/trainers/breakthrough.py | ★★★ | 学習停滞（プラトー）からの脱出、動的パラメータ調整 |
|  | **Physics Informed** | ⚠️ Exp | training/trainers/physics\_informed.py | ★★☆ | 物理法則（エネルギー保存則等）を考慮した学習 |
|  | **Knowledge Distillation** | ✅ Core | training/trainers/distillation.py | ★★★ | モデル圧縮、高速化、教師モデルからの知識転送 |
|  | **Self-Supervised** | ⚠️ Exp | training/trainers/self\_supervised.py | ★★☆ | ラベルなしデータからの高度な表現学習 |
| **統合アーキテクチャ** | **SARA Engine** | ⚠️ Exp | models/experimental/sara\_engine.py | ★★☆ | 感情・共感覚・自律性を統合したAIエージェント構築 |
| **適応・その他** | **Sleep Consolidation** | ✅ Core | cognitive\_architecture/sleep\_consolidation.py | ★★★ | 継続学習、記憶の長期定着、忘却防止 |
|  | **Structural Plasticity** | ✅ Core | evolution/structural\_plasticity.py | ★★☆ | シナプスの動的な生成と除去（配線最適化） |
|  | **On-Chip Self-Correction** | ⚠️ Exp | adaptive/on\_chip\_self\_corrector.py | ★★☆ | 推論時のリアルタイム自己修正、適応 |
|  | **Logic Gated** | ⚠️ Exp | core/layers/logic\_gated\_snn.py | ★☆☆ | ルールベースとSNNの融合、論理演算 |
|  | **Thermodynamic** | ⚠️ Exp | core/layers/thermodynamic.py | ★☆☆ | エネルギーベースモデル、物理的ゆらぎの活用 |

## **2\. 各学習方法の詳細と選択指針**

### **A. STDP (スパイクタイミング依存可塑性)**

生物学的妥当性が高い、教師なし学習の基本ルールです。

* **概要**: 前ニューロンと後ニューロンの発火タイミングの差に基づいてシナプス荷重を強化・減弱させます。  
* **選択指針**: ラベルのないデータから特徴を抽出したい場合、計算コストを低く抑えたい場合。

### **B. Forward-Forward Algorithm**

Backpropagation (誤差逆伝播法) の代替として提案された手法です。

* **概要**: 「Positive Data」と「Negative Data」を個別に流し、局所的な適合度を最大化・最小化します。  
* **選択指針**: **GPUメモリが限られている**場合や、層が非常に深いネットワークを学習させる場合。

### **C. Predictive Coding / Active Inference**

脳の予測情報処理に基づく包括的なフレームワークです。

* **Predictive Coding**: 予測誤差を最小化するように内部モデルを更新します（知覚）。  
* **Active Inference**: 予測誤差（サプライズ）を最小化するように「行動」を選択します（制御）。  
* **選択指針**: 生成モデルの作成や、外部環境と相互作用する自律エージェントの制御を行う場合。

### **D. Spike PPO / SAC (深層強化学習)**

SNNを用いた強化学習アルゴリズムです。

* **PPO**: 安定性が高く調整が容易。  
* **SAC**: 探索能力とサンプル効率に優れる。  
* **選択指針**: **ロボティクス制御**や**ゲームAI**など、複雑な行動を学習させる場合。

### **E. Advanced Trainers (高度な学習トレーナー)**

特定の課題解決のためのメタ学習手法です。

* **Breakthrough**: 学習停滞時にパラメータを動的に変化させ、局所解から脱出します。  
* **Physics Informed**: 物理法則を損失関数に組み込み、物理的に矛盾しない挙動を学習させます。  
* **Self-Supervised**: データの一部を隠して予測させるなど、ラベルなしデータから高度な特徴を学習します。  
* **選択指針**: 教師あり学習の精度向上や、物理シミュレーションの代理モデル作成時。

### **F. Knowledge Distillation & Sleep Consolidation**

知識の転送と定着に関するメカニズムです。

* **Knowledge Distillation**: 大規模モデル（教師）の出力を小規模モデル（生徒）に模倣させ、軽量化・高速化を図ります。  
* **Sleep Consolidation**: 短期記憶の内容を長期記憶ネットワークへ転送し、継続学習における破滅的忘却を防ぎます。  
* **選択指針**: モデルの軽量化（エッジAI化）や、長期間にわたる学習を行う場合。

### **G. Adaptive Mechanisms (構造的・動的適応)**

パラメータ以外の要素を変化させる適応手法です。

* **Structural Plasticity**: 不要なシナプスを削除し、有用なシナプスを新規生成します。脳の省エネ性能と学習能力の両立に寄与します。  
* **On-Chip Self-Correction**: 推論実行時（ランタイム）にリアルタイムで重みや閾値を微調整し、環境変化に適応します。  
* **選択指針**: ネットワーク構造自体の最適化や、デプロイ後の環境変化へのロバスト性が必要な場合。

### **H. SARA Engine (統合的認知アーキテクチャ)**

感情、共感覚、ホメオスタシスを統合した、プロジェクト独自の中核エンジンです。

* **概要**: 複数のニューラルモジュールを統括し、\*\*感情状態（Emotion）**や**内部欲求（Motivation）\*\*に基づいて学習率や行動方針を動的に変調します。単なる学習アルゴリズムではなく、自律エージェントの「心」を形成するシステムです。  
* **選択指針**:  
  * 「生き物」のような自律性と感情を持つエージェントを作成したい場合。  
  * 視覚・聴覚・言語などのマルチモーダル情報を共感覚的に統合処理させたい場合。

## **3\. 学習方法選択フローチャート**

```mermaid
graph TD  
    Start\[学習タスクの開始\] \--\> Q1{教師データはあるか？}

    Q1 \-- Yes \--\> Q2{タスクの種類は？}  
    Q1 \-- No \--\> Q3{環境からの報酬/FBはあるか？}

    %% 教師あり学習の分岐  
    Q2 \-- クラス分類/回帰 \--\> Q2\_1{制約条件は？}  
    Q2 \-- 生成/異常検知 \--\> PC\[Predictive Coding\]  
    Q2 \-- 物理制約あり \--\> PI\[Physics Informed Trainer\]

    Q2\_1 \-- メモリ制約あり \--\> FF\[Forward-Forward\]  
    Q2\_1 \-- モデル軽量化 \--\> KD\[Knowledge Distillation\]  
    Q2\_1 \-- 精度・速度バランス \--\> BP\[Surrogate Gradient BP\]  
      
    %% 教師なし/強化学習の分岐  
    Q3 \-- Yes (行動/制御) \--\> Q3\_1{制御の方針は？}  
    Q3 \-- No (パターン認識) \--\> Q3\_2{特徴抽出のアプローチ}

    Q3\_1 \-- 感情/自律性重視 \--\> SARA\[SARA Engine\]  
    Q3\_1 \-- 報酬最大化 \--\> RL\[Spike PPO / SAC\]  
    Q3\_1 \-- 驚き最小化 \--\> AI\[Active Inference\]

    Q3\_2 \-- 生物学的妥当性 \--\> STDP\[STDP / Hebbian\]  
    Q3\_2 \-- 高度な表現 \--\> SSL\[Self-Supervised\]

    %% 共通の課題解決  
    Stuck{学習が停滞？} \-- Yes \--\> BT\[Breakthrough Trainer\]  
    Spar{配線を最適化？} \-- Yes \--\> SP\[Structural Plasticity\]  
    Cont{継続学習？} \-- Yes \--\> Sleep\[Sleep Consolidation\]
```

## **4\. サンプルスクリプト**

各手法を試すための主要なスクリプトへのパスです。

* **標準的な学習**: scripts/training/train.py  
* **SARA Engine**: scripts/demos/brain/run\_sara\_agent.py  
* **知識蒸留**: scripts/demos/learning/run\_distillation\_demo.py  
* **構造的可塑性**: scripts/demos/brain/run\_structural\_plasticity\_demo.py  
* **Active Inference**: scripts/demos/systems/run\_active\_inference\_demo.py  
* **Forward-Forward**: scripts/demos/visual/run\_spiking\_ff\_demo.py  
* **強化学習 (RL)**: scripts/agents/run\_rl\_agent.py  
* **自己教師あり**: \`scripts/experiments/brain